{
  env: {
    env_type: gym_minigrid,
    env_task: MiniGrid-Dynamic-Obstacles,
    grid_size: 8,
    action_size: 3,
    rgb_image: false,
    goal_type: from_buffer,
    from_buffer_choice_params: {
      buffer_size: 10000,
      warmup_steps: 5000,
      prefer_unseen_states: True,
    }
    goal_achieving_criterion: position,
    video_path: artifacts/video/,
    intrinsic_episodic_reward: true,
    random_network_distillation_reward: {
      device: "cuda:0",
      target: {
        state_encoder_type: simple_mlp,
        hidden_layers_sizes: [128, 128],
        head: {
            hidden_size: 64
        }
      },
      predictor: {
        state_encoder_type: simple_mlp,
        hidden_layers_sizes: [128, 128],
        head: {
            hidden_size: 64
        }
      },
    }
    state_distance_network_params: {
        path: artifacts/models/minigrid_state_distance_encoder_raw.p,
        device: "cuda:0",
        threshold: 1
    }
  },

  worker: {
    state_encoder_type: dummy_raw,
    head: {
        hidden_size: 64
    }
  },

  master: {
    state_encoder_type: dummy_raw,
    head: {
        hidden_size: []
    },
    emb_size: 64
  }

  agent: {
    algorithm: DQN,
    device: "cuda:0",
    batch_size: 128,
    update_step: 4,
    buffer_size: 100000,
    learning_rate: 0.001,
    gamma: 0.9,
    eps_start: 1,
    eps_end: 0.1,
    eps_decay: 0.995,
    tau: 0.001
  },

  training: {
    reward: fair_goal_achievement,
    n_episodes: 3000,
    verbose: 100,
    max_steps: 256,
    validation: true,
  },

  seed: 42,

  outputs: {
    save_example: true,
    path: artifacts/models/minigrid_worker.p,
  }
}
