{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f686495a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "881b87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34945003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConfigTree([('env',\n",
       "             ConfigTree([('env_type', 'nle'),\n",
       "                         ('video_path', 'artifacts/video/'),\n",
       "                         ('goal_achieving_criterion', 'position'),\n",
       "                         ('goal_type', 'from_current_episode'),\n",
       "                         ('from_buffer_choice_params',\n",
       "                          ConfigTree([('buffer_size', 100000),\n",
       "                                      ('warmup_steps', 10)])),\n",
       "                         ('action_size', 23)])),\n",
       "            ('worker',\n",
       "             ConfigTree([('state_encoder_type', 'net_hack_encoder'),\n",
       "                         ('use_lstm', False),\n",
       "                         ('head', ConfigTree([('hidden_size', 64)]))])),\n",
       "            ('master',\n",
       "             ConfigTree([('state_encoder_type', 'net_hack_encoder'),\n",
       "                         ('use_lstm', False),\n",
       "                         ('head', ConfigTree([('hidden_size', [])])),\n",
       "                         ('emb_size', 64)])),\n",
       "            ('seed', 42),\n",
       "            ('outputs',\n",
       "             ConfigTree([('save_example', True),\n",
       "                         ('path', 'artifacts/models/minigrid_worker.p')]))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyhocon import ConfigFactory\n",
    "\n",
    "config = ConfigFactory.parse_file('../experiments/conf/nle_draft.hocon')\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e96d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5883215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../experiments')\n",
    "\n",
    "from train_worker import *\n",
    "\n",
    "state_encoder, goal_state_encoder = get_encoders(config)\n",
    "net = get_master_worker_net(state_encoder, goal_state_encoder, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e99e0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5b5332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine import BaseActorModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96e4ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderActor(BaseActorModule):\n",
    "    def __init__(self, state_encoder, epsilon=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = state_encoder\n",
    "\n",
    "        # for updating the exploration epsilon in the clones\n",
    "        self.register_buffer('epsilon', torch.tensor(epsilon))\n",
    "\n",
    "    def forward(self, obs, act, rew, fin, *, hx=None, stepno=None, virtual=False):\n",
    "        qv, hx = self.encoder(obs), ()\n",
    "        val, actions = qv.max(dim=-1)\n",
    "\n",
    "        if self.training:\n",
    "            *head, n_actions = qv.shape\n",
    "            actions = actions.where(\n",
    "                torch.rand(head, device=self.epsilon.device).gt(self.epsilon),\n",
    "                torch.randint(n_actions, size=head, device=self.epsilon.device))\n",
    "\n",
    "        return actions, hx, dict(q=qv, value=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c51cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e878e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba8ed78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e30c29f8",
   "metadata": {},
   "source": [
    "### D-DQN loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829b01a4",
   "metadata": {},
   "source": [
    "Service functions for the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0046223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.utils.plyr import apply, suply, xgetitem\n",
    "\n",
    "\n",
    "def timeshift(state, *, shift=1):\n",
    "    \"\"\"Get current and shfited slices of nested objects.\"\"\"\n",
    "    # use xgetitem to lett None through\n",
    "    # XXX `curr[t]` = (x_t, a_{t-1}, r_t, d_t), t=0..T-H\n",
    "    curr = suply(xgetitem, state, index=slice(None, -shift))\n",
    "\n",
    "    # XXX `next[t]` = (x_{t+H}, a_{t+H-1}, r_{t+H}, d_{t+H}), t=0..T-H\n",
    "    next = suply(xgetitem, state, index=slice(shift, None))\n",
    "\n",
    "    return curr, next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9af740",
   "metadata": {},
   "source": [
    "Double DQN loss for contiguous trajectory fragements. \n",
    "\n",
    "* `.state[t+1].rew` -- $r_{t+1}$\n",
    "* `.state[t+1].fin` -- $d_{t+1}$\n",
    "* `.state[t+1].act` -- $a_t$ which caused $\n",
    "    (s_t, z_t, a_t) \\longrightarrow (s_{t+1}, z_{t+1}, r_{t+1}, d_{t+1})\n",
    "$\n",
    "* `_, _, fragment.actor[t] = actor(.state[t])` -- $\n",
    "    q(z_t, h_t, \\cdot; \\theta_{\\text{old}})\n",
    "$ -- used for rollout collection\n",
    "* `_, _, info_module[t] = module(.state[t])` -- $\n",
    "    q(z_t, h_t, \\cdot; \\theta)\n",
    "$ -- the current q-function, producing $(h_t)_{t=0}^{T+1}$\n",
    "* `_, _, info_target[t] = target(.state[t])` -- $\n",
    "    q(z_t, h_t, \\cdot; \\theta_-)\n",
    "$ -- the q-target with $(h^-_t)_{t=0}^{T+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8bb258",
   "metadata": {},
   "source": [
    "The current q-network minimizes the $\\mathrm{TD}(0)$-error is\n",
    "\n",
    "$$\n",
    "\\delta_t(\\theta)\n",
    "    = \\bigl(\n",
    "        r_{t+1}\n",
    "        + \\gamma 1_{\\{\\neg d_{t+1}\\}} v^*(z_{t+1})\n",
    "    \\bigr) - q(z_t, h_t, a_t; \\theta)\n",
    "    \\,, $$\n",
    "\n",
    "where the approximate state value estimate $\n",
    "    v^*(z_{t+1})\n",
    "$ is one of\n",
    "* `Q-learning`: $\n",
    "    \\max_a q(z_{t+1}, h_{t+1}, a; \\theta)\n",
    "$\n",
    "* `DQN`: $\n",
    "    \\max_a q(z_{t+1}, h_{t+1}, a; \\theta_-)\n",
    "$\n",
    "* `double DQN`: $\n",
    "    q(z_{t+1}, h_{t+1}, \\hat{a}_{t+1}; \\theta_-)\n",
    "$ for $\n",
    "    \\hat{a}_{t+1} = \\arg \\max_a Qq(z_{t+1}, h_{t+1}, a; \\theta)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbae1aa7",
   "metadata": {},
   "source": [
    "One of the works realted to DQN learning of recurrent agents is [Kapturowski et al. (2018)](https://openreview.net/forum?id=r1lyTjAqYX), who propose to use a burn-in period in\n",
    "the contiguous trajectory fragment in order to compenaste for the representation drift,\n",
    "due to differnet RNN parameters $\\theta_-$ and $\\theta$.\n",
    "\n",
    "There is no clear-cut evidence suggesting that the hidden recurrent sequences $h_t$\n",
    "and $h^-_t$ yield significantly different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceaf2134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# @torch.enable_grad()\n",
    "def ddq_learn(fragment, module, *, gamma=0.95, target=None, double=False):\n",
    "    r\"\"\"Compute the Double-DQN loss over a _contiguous_ fragment of a trajectory.\n",
    "\n",
    "    Details\n",
    "    -------\n",
    "    In Q-learning the action value function minimizes the TD-error\n",
    "\n",
    "    $$\n",
    "        r_{t+1}\n",
    "            + \\gamma 1_{\\neg d_{t+1}} v^*(z_{t+1})\n",
    "            - q(z_t, a_t; \\theta)\n",
    "        \\,, $$\n",
    "\n",
    "    w.r.t. Q-network parameters $\\theta$ where $z_t$ is the actionable state,\n",
    "    $r_{t+1}$ is the reward for $s_t \\to s_{t+1}$ transition. The value of\n",
    "    $z_t$ include the current observation $x_t$ and the recurrent state $h_t$,\n",
    "    the last action $a_{t-1}$, the last reward $r_t$, and termination flag\n",
    "    $d_t$.\n",
    "\n",
    "    In the classic Q-learning there is no target network and the next state\n",
    "    optimal state value function is bootstrapped using the current Q-network\n",
    "    (`module`):\n",
    "\n",
    "    $$\n",
    "        v^*(z_{t+1})\n",
    "            \\approx \\max_a q(z_{t+1}, a; \\theta)\n",
    "        \\,. $$\n",
    "\n",
    "    The DQN method, proposed by\n",
    "\n",
    "        [Minh et al. (2013)](https://arxiv.org/abs/1312.5602),\n",
    "\n",
    "    uses a secondary Q-network (`target`) to estimate the value of the next\n",
    "    state:\n",
    "\n",
    "    $$\n",
    "        v^*(z_{t+1})\n",
    "            \\approx \\max_a q(z_{t+1}, a; \\theta^-)\n",
    "        \\,, $$\n",
    "\n",
    "    where $\\theta^-$ are the frozen parameters of the target Q-network. The\n",
    "    Double DQN algorithm of\n",
    "\n",
    "        [van Hasselt et al. (2015)](https://arxiv.org/abs/1509.06461)\n",
    "\n",
    "    unravels the $\\max$ operator as\n",
    "    $\n",
    "        \\max_k u_k \\equiv u_{\\arg \\max_k u_k}\n",
    "    $\n",
    "    and replaces the outer $u$ with the Q-values of the target Q-network, while\n",
    "    computing the inner $u$ (inside the $\\arg\\max$) with the current Q-network.\n",
    "    Specifically, the Double DQN value estimate is\n",
    "\n",
    "    $$\n",
    "        v^*(z_{t+1})\n",
    "            \\approx q(z_{t+1}, \\hat{a}_{t+1}; \\theta^-)\n",
    "            \\,,\n",
    "            \\hat{a}_{t+1}\n",
    "                = \\arg \\max_a q(z_{t+1}, a; \\theta)\n",
    "        \\,, $$\n",
    "\n",
    "    for $\n",
    "        \\hat{a}_{t+1}\n",
    "            = \\arg \\max_a q(s_{t+1}, a; \\theta)\n",
    "    $ being the action taken by the current Q-network $\\theta$ at $z_{t+1}$.\n",
    "\n",
    "    Recurrent DQN\n",
    "    -------------\n",
    "    The key problem with the recurrent state $h_t$ in $z_t$ is its representaion\n",
    "    drift: the endogenous states used for collecting trajectory data during the\n",
    "    rollout are produced by an actor with stale perameters $\\theta_{\\text{old}}$,\n",
    "    and thus might have high discrepancy with the recurrent state produced by\n",
    "    the current Q-network $\\theta$ or the target $\\theta-_$. To mitigate this\n",
    "        \n",
    "        [Kapturowski et al. (2018)](https://openreview.net/forum?id=r1lyTjAqYX)\n",
    "    \n",
    "    proposed to spend a slice `burnin` of the recorded trajectory on\n",
    "    aligning the recurrent representation. Specifically, starting with $h_0$\n",
    "    (contained in `fragment.hx`) they propose to launch two sequences $h_t$\n",
    "    and $h^-_t$ from the same $h^-_0 = h_0$ using $q(\\cdot; \\theta)$ and\n",
    "    $q(\\cdot; \\theta^-)$, respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    trajectory, hx = fragment.state, fragment.hx\n",
    "    obs, act, rew, fin = trajectory.obs, trajectory.act, trajectory.rew, trajectory.fin\n",
    "\n",
    "    # get $Q(z_t, h_t, \\cdot; \\theta)$ for all t=0..T\n",
    "    _, _, info_module = module(\n",
    "        obs, act, rew, fin, hx=hx, stepno=trajectory.stepno)\n",
    "\n",
    "    # get the next state `state[t+1]` $z_{t+1}$ to access $a_t$\n",
    "    state_next = suply(xgetitem, trajectory, index=slice(1, None))\n",
    "\n",
    "    # $\\hat{A}_t$, the module's response to current and next state,\n",
    "    #  contains the q-values. `curr` is $q(z_t, h_{t+1}, \\cdot; \\theta)$\n",
    "    #  and `next` is $q(z_{t+1}, h_{t+1}, \\cdot; \\theta)$ is `next`.\n",
    "    info_module_curr, info_module_next = timeshift(info_module)\n",
    "\n",
    "    # get $q(z_t, h_t, a_t; \\theta)$ for all t=0..T-1\n",
    "    q_replay = info_module_curr['q'].gather(-1, state_next.act.unsqueeze(-1))\n",
    "\n",
    "    # get $\\hat{v}_{t+1}(z_{t+1}) = ...$\n",
    "    with torch.no_grad():\n",
    "        if target is None:\n",
    "            # get $... = \\max_a Q(z_{t+1}, h_{t+1}, a; \\theta)$\n",
    "            q_value = info_module_next['q'].max(dim=-1, keepdim=True).values\n",
    "\n",
    "        else:\n",
    "            _, _, info_target = target(\n",
    "                obs, act, rew, fin, hx=hx, stepno=trajectory.stepno)\n",
    "\n",
    "            info_target_next = suply(xgetitem, info_target, index=slice(1, None))\n",
    "            if not double:\n",
    "                # get $... = \\max_a Q(z_{t+1}, h^-_{t+1}, a; \\theta^-)$\n",
    "                q_value = info_target_next['q'].max(dim=-1, keepdim=True).values\n",
    "\n",
    "            else:\n",
    "                # get $\\hat{a}_{t+1} = \\arg \\max_a Q(z_{t+1}, h_{t+1}, a; \\theta)$\n",
    "                hat_act = info_module_next['q'].max(dim=-1).indices.unsqueeze(-1)\n",
    "\n",
    "                # get $... = Q(z_{t+1}, h^-_{t+1}, \\hat{a}_{t+1}; \\theta^-)$\n",
    "                q_value = info_target_next['q'].gather(-1, hat_act)\n",
    "\n",
    "        # get $r_{t+1} + \\gamma 1_{d_{t+1}} \\hat{v}_{t+1}(z_{t+1})$ using inplace ops\n",
    "        q_value.masked_fill_(state_next.fin.unsqueeze(-1), 0.)\n",
    "        q_value.mul_(gamma).add_(state_next.rew.unsqueeze(-1))\n",
    "\n",
    "    # td-error ell-2 loss\n",
    "    return F.mse_loss(q_replay, q_value, reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab8352",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fd19da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c84768c",
   "metadata": {},
   "source": [
    "### Run!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e336a5e",
   "metadata": {},
   "source": [
    "prepare the optimizer for the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad23b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.6\n",
    "use_target = False\n",
    "use_double = False\n",
    "\n",
    "# `target` does not work for some reason at all with taxi, maybe the freeze schedule is off?\n",
    "#  or contiguous fragments work to the detrement of learning\n",
    "\n",
    "# `duelling` also fails for both target and double, and sorta for ordinary q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842dc177",
   "metadata": {},
   "source": [
    "Initialize the learner and the environment factories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d06c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from train_worker import gen_navigation_env, gen_env\n",
    "\n",
    "factory_eval = partial(gen_navigation_env, conf=config['env'])\n",
    "factory = partial(gen_navigation_env, conf=config['env'])\n",
    "\n",
    "learner = EncoderActor(net)\n",
    "\n",
    "learner.train()\n",
    "device_ = torch.device('cpu')  #  torch.device('cuda:0')\n",
    "learner.to(device=device_)\n",
    "\n",
    "optim = torch.optim.SGD(learner.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10023d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2c1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f7229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82730b1a",
   "metadata": {},
   "source": [
    "Initialize the sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "991c289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "T, B = 25, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c4d3371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout import same\n",
    "\n",
    "batchit = same.rollout(\n",
    "    [factory() for _ in range(B)],\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    sticky=False,\n",
    "    device=device_,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c298875",
   "metadata": {},
   "source": [
    "from rlplay.engine.rollout import single\n",
    "\n",
    "batchit = single.rollout(\n",
    "    factory,\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    n_envs=B,\n",
    "    sticky=False,\n",
    "    clone=True,\n",
    "    close=False,\n",
    "    device=device_,\n",
    "    start_method='fork',  # fork in notebook for macos, spawn in linux\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07978f95",
   "metadata": {},
   "source": [
    "from rlplay.engine.rollout import multi\n",
    "\n",
    "batchit = multi.rollout(\n",
    "    factory,\n",
    "    learner,\n",
    "    n_steps=T,\n",
    "    n_actors=8,\n",
    "    n_per_actor=B,\n",
    "    n_buffers=16,\n",
    "    n_per_batch=2,\n",
    "    sticky=False,\n",
    "    pinned=False,\n",
    "    clone=True,\n",
    "    close=False,\n",
    "    device=device_,\n",
    "    start_method='fork',  # fork in notebook for macos, spawn in linux\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9a0a4",
   "metadata": {},
   "source": [
    "A generator of evaluation rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a0f3229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlplay.engine.rollout.evaluate import evaluate\n",
    "\n",
    "test_it = evaluate(factory_eval, learner, n_envs=4, n_steps=500,\n",
    "                   clone=False, device=device_, start_method='fork')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846d8c9",
   "metadata": {},
   "source": [
    "Implement your favourite training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1ab50a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 14/400 [28:59<13:32:52, 126.35s/it]Process ForkProcess-1:\n",
      "  4%|▎         | 14/400 [30:53<14:11:41, 132.39s/it]Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/nikita/ML/work_repo/rlplay/rlplay/engine/rollout/evaluate.py\", line 102, in p_evaluate\n",
      "    ctrl.alpha.rx.recv()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-72c259c09d0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_target\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         loss = ddq_learn(batch, learner, target=target,\n\u001b[1;32m     17\u001b[0m                          gamma=gamma, double=use_double)\n",
      "\u001b[0;32m~/ML/work_repo/rlplay/rlplay/engine/rollout/same.py\u001b[0m in \u001b[0;36mrollout\u001b[0;34m(envs, actor, n_steps, sticky, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# collect the fragment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfragment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msticky\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msticky\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# move to the specified device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/rllr-EAe8DutN/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML/work_repo/rlplay/rlplay/engine/core.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(envs, actor, fragment, context, sticky, device)\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;31m# REACT: $(a_t, h_{t+1})$ are actor's reaction to `.state[t]` and `hx`,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;31m#  i.e. $(t, x_t, a_{t-1}, r_t, d_t)$, and $h_t$, respectively.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         \u001b[0mact_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_actor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpyt_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvirtual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m         \u001b[0;31m# XXX The actor SHOULD respect time and batch dims of the inputs,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;31m#  except `hx`, but SHOULD NOT change or update anything in-place.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML/work_repo/rlplay/rlplay/engine/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, stepno, obs, act, rew, fin, hx, virtual)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0mSHOULD\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0monly\u001b[0m \u001b[0mnewly\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \"\"\"\n\u001b[0;32m--> 304\u001b[0;31m         return self(obs, act=act, rew=rew, fin=fin, stepno=stepno,\n\u001b[0m\u001b[1;32m    305\u001b[0m                     hx=hx, virtual=virtual)\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/rllr-EAe8DutN/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-e980c7009760>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs, act, rew, fin, hx, stepno, virtual)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstepno\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvirtual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mqv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/rllr-EAe8DutN/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML/work_repo/rllr/rllr/models/dqn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mstates_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/rllr-EAe8DutN/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML/work_repo/rllr/rllr/models/encoders/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, states_and_goals)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_and_goals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mstate_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_and_goals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'goal_emb'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstates_and_goals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML/work_repo/rllr/rllr/models/encoders/nle_encoders.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, env_outputs, core_state)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mglyphs_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglyphs_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# -- TODO: slow?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m# -- [B x W x H x K]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mglyphs_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_representation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglyphs_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# -- [B x K']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/rllr-EAe8DutN/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/rllr-EAe8DutN/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/rllr-EAe8DutN/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/rllr-EAe8DutN/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/rllr-EAe8DutN/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    393\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 395\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    396\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import copy\n",
    "from math import log, exp\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "# the training loop\n",
    "losses, rewards = [], []\n",
    "decay = -log(2) / 50  # exploration epsilon halflife\n",
    "for epoch in tqdm.tqdm(range(400)):\n",
    "    # freeze the target for q\n",
    "    target = copy.deepcopy(learner) if use_target else None\n",
    "\n",
    "    for j, batch in zip(range(100), batchit):\n",
    "        loss = ddq_learn(batch, learner, target=target,\n",
    "                         gamma=gamma, double=use_double)\n",
    "\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        grad = clip_grad_norm_(learner.parameters(), max_norm=1e3)\n",
    "        optim.step()\n",
    "\n",
    "        losses.append(dict(\n",
    "            loss=float(loss), grad=float(grad),\n",
    "        ))\n",
    "\n",
    "    learner.epsilon.mul_(exp(decay)).clip_(0.1, 1.0)\n",
    "\n",
    "    # fetch the evaluation results lagged by one inner loop!\n",
    "    rewards.append(next(test_it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0633c36c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# close the generators\n",
    "batchit.close()\n",
    "test_it.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a84241a",
   "metadata": {},
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce115d82",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(records):\n",
    "    \"\"\"collate identically keyed dicts\"\"\"\n",
    "    out, n_records = {}, 0\n",
    "    for record in records:\n",
    "        for k, v in record.items():\n",
    "            out.setdefault(k, []).append(v)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "data = {k: numpy.array(v) for k, v in collate(losses).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(data['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01581b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(data['grad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3128f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = numpy.stack(rewards, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ffd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd6ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = numpy.median(rewards, axis=-1), rewards.std(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5545163",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fi, ax = plt.subplots(1, 1, figsize=(4, 2), dpi=300)\n",
    "\n",
    "ax.plot(numpy.mean(rewards, axis=-1))\n",
    "ax.plot(numpy.median(rewards, axis=-1))\n",
    "ax.plot(numpy.min(rewards, axis=-1))\n",
    "ax.plot(numpy.std(rewards, axis=-1))\n",
    "# ax.plot(m+s * 1.96)\n",
    "# ax.plot(m-s * 1.96)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9856d6a4",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace0ac8",
   "metadata": {},
   "source": [
    "The ultimate evaluation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d372a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from rlplay.engine import core\n",
    "\n",
    "with factory_eval() as env:\n",
    "    learner.eval()\n",
    "    eval_rewards, info = core.evaluate([\n",
    "        env\n",
    "    ], learner, render=True, n_steps=1e2, device=device_)\n",
    "\n",
    "print(sum(eval_rewards))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4716caad",
   "metadata": {},
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e33904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdca423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36570167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1795ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d316d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../experiments')\n",
    "\n",
    "from train_worker import gen_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec47463",
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_reproducibility_on(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b4f26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gen_navigation_env(config['env'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e69889",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c3afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(20):\n",
    "    state, _, _, _ = env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f31a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a323f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rllr.utils import convert_to_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f53b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = convert_to_torch([state, state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f6847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = convert_to_torch([state['state']] * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb00ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_encoder(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6095a739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Size([2, 25]) torch.Size([2, 21, 79])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feef8ace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408c9f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906471cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c1ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = convert_to_torch([state['state'], state['goal_state']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1c0447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec42ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9490a758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e42ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4cf233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fdfbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46578574",
   "metadata": {},
   "outputs": [],
   "source": [
    "state['goal_state']['position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.last_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6213c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.buffer_random_choice().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d15990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a8eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc89599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import nle\n",
    "env_ = gym.make(\"NetHackScore-v0\", observation_keys=(\"glyphs\", \"blstats\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77f717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80acb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env_.last_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6fdaba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3416a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c593e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1a5a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3550a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "state['goal_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48c90e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(state['glyphs'].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41653080",
   "metadata": {},
   "outputs": [],
   "source": [
    "glyphs = state[\"glyphs\"]\n",
    "T, B, *_ = glyphs.shape\n",
    "\n",
    "blstats = state[\"blstats\"]\n",
    "blstats.view(T * B, -1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e6084",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35de2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ada6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f860fe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "states0 = env.reset()  # each reset generates a new dungeon\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f4012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "states0['blstats'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "states0['glyphs'][6, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b71e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "states1, _, _, _ = env.step(1)  # move agent '@' north\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7b3dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "states1['blstats'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af51e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "states1['glyphs'][5, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0d1197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
