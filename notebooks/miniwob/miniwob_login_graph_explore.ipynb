{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "device = \"cuda:3\"\n",
    "\n",
    "# Setup: and Selenium chrome driver to PATH and set MINIWOB_BASE_URL env variable to the directory with HTML task files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE URL: file:///mnt/akostin/home/akostin/prjs/miniwob-plusplus/html/\n"
     ]
    }
   ],
   "source": [
    "from miniwob.environment import MiniWoBEnvironment\n",
    "from miniwob.screenshot import pil_to_numpy_array\n",
    "\n",
    "task_name = 'login-user'\n",
    "base_url = os.environ.get('MINIWOB_BASE_URL')\n",
    "print('BASE URL:', base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_MAX, Y_MAX = 160, 210\n",
    "DOM_TEXT_MAX_TOKENS = 8\n",
    "GOAL_MAX_TOKENS = 32\n",
    "GOAL_MAX_WORDS = 25\n",
    "DOM_MAX_EDGES = 20\n",
    "DOM_MAX_VERTICES = 20\n",
    "EXPLORE_REWARD = 0.1\n",
    "\n",
    "username_idx = 4\n",
    "passwd_idx = 10\n",
    "username_element_id = 1\n",
    "password_element_id = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f68bce47f10>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f4573985440fcbfc0eb425703daf4598\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f68dd85ad60>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f4573985440fcbfc0eb425703daf4598\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f68dc7d67c0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/f4573985440fcbfc0eb425703daf4598\n"
     ]
    }
   ],
   "source": [
    "from miniwob.action import  MiniWoBElementClick, MiniWoBType\n",
    "import numpy as np\n",
    "\n",
    "env = MiniWoBEnvironment(task_name, seeds=[1], num_instances=1, base_url=base_url, headless=True)\n",
    "state = env.reset(record_screenshots=True, seeds=[11])[0]\n",
    "print(len(state.tokens))\n",
    "tags = set()\n",
    "for elem in state.dom_elements:\n",
    "    tags.add(elem.tag)\n",
    "tag2id = {t: i for i, t in enumerate(tags)}\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym miniwob wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "from miniwob.action import MiniWoBElementClick, MiniWoBType\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MiniWobClickElementWrapper(gym.Wrapper):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        bert_model = 'bert-base-cased'\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "        self.observation_space = gym.spaces.Dict({\"goal_state\": gym.spaces.Box(0, \n",
    "                                                                               self.tokenizer.vocab_size,\n",
    "                                                                               (GOAL_MAX_TOKENS, ),\n",
    "                                                                               dtype=np.int64),\n",
    "                                                  \"img_state\": gym.spaces.Box(0,\n",
    "                                                                              255,\n",
    "                                                                              (150, 150, 3),\n",
    "                                                                              dtype=np.uint8),\n",
    "                                                  \"dom_simple_feats\": gym.spaces.Box(-1,\n",
    "                                                                                     1,\n",
    "                                                                                     (DOM_MAX_VERTICES, 3),\n",
    "                                                                                     dtype=np.float32),\n",
    "                                                  \"dom_text_tokens\": gym.spaces.Box(0,\n",
    "                                                                                    self.tokenizer.vocab_size,\n",
    "                                                                                    (DOM_MAX_VERTICES, DOM_TEXT_MAX_TOKENS),\n",
    "                                                                                    dtype=np.int64),\n",
    "                                                  \"dom_tags\": gym.spaces.Box(0,\n",
    "                                                                             len(tags)-1,\n",
    "                                                                             (DOM_MAX_VERTICES,),\n",
    "                                                                             dtype=np.int64),\n",
    "                                                  \"n_goal_words\": gym.spaces.Box(0,\n",
    "                                                                                 GOAL_MAX_WORDS,\n",
    "                                                                                 (1,),\n",
    "                                                                                 dtype=np.int64),\n",
    "                                                  \"action_mask\": gym.spaces.Box(0,\n",
    "                                                                                1,\n",
    "                                                                                (DOM_MAX_VERTICES + GOAL_MAX_WORDS,),\n",
    "                                                                                dtype=np.int64)})\n",
    "        \n",
    "        self.past_states = [set() for _ in range(self.num_instances)]\n",
    "        self.curr_states = [[0 for _ in range(DOM_MAX_VERTICES*2)] for _ in range(self.num_instances)]\n",
    "        self.inst_verts = [None for _ in range(self.num_instances)]\n",
    "        self.text_tokens = [None for _ in range(self.num_instances)]\n",
    "        self.dom_keys = [key for key in self.observation_space if key.startswith(\"dom\")] + \\\n",
    "                        [\"n_goal_words\"] + [\"action_mask\"]\n",
    "        self.action_space = gym.spaces.Discrete(DOM_MAX_VERTICES + GOAL_MAX_WORDS)\n",
    "    \n",
    "    def _ob_to_dom(self, ob, i):\n",
    "        max_length = DOM_TEXT_MAX_TOKENS\n",
    "        if ob is None:\n",
    "            self.inst_verts[i] = None\n",
    "            self.text_tokens[i] = None\n",
    "            return {\n",
    "                \"dom_simple_feats\": -torch.ones((1, DOM_MAX_VERTICES, 3), dtype=torch.float32),\n",
    "                \"dom_text_tokens\": torch.zeros((1, DOM_MAX_VERTICES, DOM_TEXT_MAX_TOKENS), dtype=torch.int64),\n",
    "                \"dom_tags\": torch.zeros((1, DOM_MAX_VERTICES), dtype=torch.int64),\n",
    "                \"n_goal_words\": torch.zeros((1,1), dtype=torch.int64),\n",
    "                \"action_mask\": torch.ones((1, DOM_MAX_VERTICES + GOAL_MAX_WORDS), dtype=torch.int64)\n",
    "            }\n",
    "        else:\n",
    "            self.inst_verts[i] = ob.dom_elements\n",
    "            self.text_tokens[i] = ob.tokens\n",
    "            tokens = []\n",
    "            feats = []\n",
    "            tags_ = []\n",
    "            for elem in ob.dom_elements:\n",
    "                elem_text = elem.text if elem.text else \"\"\n",
    "                elem_tokens = self.tokenizer(elem_text,\n",
    "                                             padding='max_length',\n",
    "                                             max_length=DOM_TEXT_MAX_TOKENS,\n",
    "                                             truncation=True,\n",
    "                                             return_tensors=\"pt\")[\"input_ids\"]\n",
    "                tokens.append(elem_tokens)\n",
    "                elem_feats = torch.tensor([elem.focused, bool(elem.value), elem.tampered],\n",
    "                                          dtype=torch.float32)\n",
    "                feats.append(elem_feats)\n",
    "                tags_.append(torch.tensor([tag2id[elem.tag]], dtype=torch.int64))\n",
    "            \n",
    "            delta_v = DOM_MAX_VERTICES - len(tokens)\n",
    "            available_dom_click = [0 for _ in range(len(tokens))] + [1 for _ in range(delta_v)]\n",
    "            \n",
    "            if self.typeble_elem_focused(i) and not self.elem_have_value(i):\n",
    "            #if not self.elem_have_value(i):\n",
    "                available_type_tokens = [0 for _ in range(len(self.text_tokens[i]))] + \\\n",
    "                                        [1 for _ in range(GOAL_MAX_WORDS - len(self.text_tokens[i]))]\n",
    "            else:\n",
    "                available_type_tokens = [1 for _ in range(GOAL_MAX_WORDS)]\n",
    "            available_actions = available_dom_click + available_type_tokens\n",
    "            \n",
    "            tokens += [torch.zeros((1, DOM_TEXT_MAX_TOKENS), dtype=torch.int64) for _ in range(delta_v)]\n",
    "            feats += [-torch.ones((3,), dtype=torch.float32) for _ in range(delta_v)]\n",
    "            tags_ += [torch.zeros((1,), dtype=torch.int64) for _ in range(delta_v)]\n",
    "            \n",
    "            return{\n",
    "                \"dom_text_tokens\": torch.cat(tokens, dim=0).unsqueeze(0),\n",
    "                \"dom_simple_feats\": torch.stack(feats, dim=0).unsqueeze(0),\n",
    "                \"dom_tags\": torch.cat(tags_, dim=0).unsqueeze(0),\n",
    "                \"n_goal_words\": torch.tensor([len(self.text_tokens[i])]).unsqueeze(0),\n",
    "                \"action_mask\": torch.tensor(available_actions).unsqueeze(0)\n",
    "            }\n",
    "    \n",
    "    def typeble_elem_focused(self, i):\n",
    "        focused_elem = self.n_element_focused(i)\n",
    "        if focused_elem in [username_element_id, password_element_id]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def elem_have_value(self, i):\n",
    "        focused_elem = self.n_element_focused(i)\n",
    "        if not focused_elem is None:\n",
    "            s = self.curr_states[i]\n",
    "            value = [int(s[i]) for i in range(1, len(s), 2)]\n",
    "            return bool(value[focused_elem])\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def n_element_focused(self, i):\n",
    "        s = self.curr_states[i]\n",
    "        focus = [int(s[i]) for i in range(0, len(s), 2)]\n",
    "        if 1 in focus:\n",
    "            return focus.index(1)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def _ob_to_token(self, ob, i):\n",
    "        max_length = GOAL_MAX_TOKENS\n",
    "        if ob is None:\n",
    "            return torch.zeros(1, max_length)\n",
    "        else: \n",
    "            return self.tokenizer(ob.tokens,  \n",
    "                                  padding='max_length', \n",
    "                                  max_length = max_length, truncation=True,          \n",
    "                                  return_tensors=\"pt\",\n",
    "                                  is_split_into_words=True)['input_ids']\n",
    "            \n",
    "    def _ob_to_image(self, ob):\n",
    "        if ob is None:\n",
    "            return torch.zeros(1, 150, 150, 3)\n",
    "        else:\n",
    "            return torch.tensor(pil_to_numpy_array(ob.screenshot.resize([150, 150]))).unsqueeze(0)\n",
    "        \n",
    "    \n",
    "    def _to_miniwob_actions(self, actions):\n",
    "        \n",
    "        actions = actions.squeeze()\n",
    "        miniwob_actions = []\n",
    "        \n",
    "        for i in range(self.num_instances):\n",
    "            if self.instances[i].get_metadata()['done']:\n",
    "                miniwob_actions.append(None)\n",
    "            else:\n",
    "                if self.inst_verts[i] is None:\n",
    "                    miniwob_actions.append(None)\n",
    "                else:\n",
    "                    v_n = actions[i].item()\n",
    "                    if v_n < DOM_MAX_VERTICES:\n",
    "                        element = self.inst_verts[i][v_n]\n",
    "                        miniwob_actions.append(MiniWoBElementClick(element))\n",
    "                    else:\n",
    "                        text = self.text_tokens[i][v_n - DOM_MAX_VERTICES]\n",
    "                        miniwob_actions.append(MiniWoBType(text))\n",
    "        return miniwob_actions\n",
    "    \n",
    "    def get_explore_reward(self, obs):\n",
    "        explore_rewards = [0 for _ in range(self.num_instances)]\n",
    "        for i, states in enumerate(self.past_states):\n",
    "            t = \"\".join([str(int(x)) for x in obs[\"dom_simple_feats\"][i][:, :-1].flatten()])\n",
    "            t = t.split('-')[0]\n",
    "            if t in states:\n",
    "                explore_rewards[i] -= EXPLORE_REWARD\n",
    "            else:\n",
    "                explore_rewards[i] += EXPLORE_REWARD\n",
    "        return explore_rewards\n",
    "\n",
    "    def observation(self, obs):\n",
    "        goals = torch.cat([self._ob_to_token(obs[i], i) for i in range(len(obs))], dim=0)\n",
    "        imgs = torch.cat([self._ob_to_image(ob) for ob in obs], dim=0)\n",
    "        doms = [self._ob_to_dom(obs[i], i) for i in range(len(obs))]\n",
    "        doms_states = {key: torch.cat([dom[key] for dom in doms], dim=0) for key in self.dom_keys}\n",
    "        return {'img_state': imgs, 'goal_state': goals, **doms_states}\n",
    "    \n",
    "    def update_past_states(self, obs):\n",
    "        for i, states in enumerate(self.past_states):\n",
    "            s = \"\".join([str(int(x)) for x in obs[\"dom_simple_feats\"][i][:, :-1].flatten()])\n",
    "            s = s.split('-')[0]\n",
    "            self.past_states[i].add(s)\n",
    "            self.curr_states[i] = s\n",
    "    \n",
    "    def reset(self):\n",
    "        obs = self.observation(self.env.reset(record_screenshots=True))\n",
    "        self.update_past_states(obs)\n",
    "        return obs\n",
    "    \n",
    "    def step(self, actions):\n",
    "        miniwob_actions = self._to_miniwob_actions(actions)\n",
    "        obs, rewards, dones, infos = self.env.step(miniwob_actions)\n",
    "        obs = self.observation(obs)\n",
    "        ####\n",
    "        explore_rewards = self.get_explore_reward(obs)\n",
    "        rewards = [a + b for a, b in zip(rewards, explore_rewards)]\n",
    "        infos['n'][0]['exploration_rewards'] = explore_rewards\n",
    "        ####\n",
    "        self.update_past_states(obs)\n",
    "        \n",
    "        for i, instance in enumerate(self.env.instances):\n",
    "            if instance.get_metadata()['done']:\n",
    "                instance.begin_task()\n",
    "                self.past_states[i] = set()\n",
    "        \n",
    "        return obs, torch.tensor([rewards]).T, torch.tensor([dones]).T, infos['n']\n",
    "\n",
    "\n",
    "class EpisodeInfoWrapper(gym.Wrapper):\n",
    "    \n",
    "    def __init__(self, env, n):\n",
    "        super(EpisodeInfoWrapper, self).__init__(env)\n",
    "        self.episode_reward = np.zeros(n)\n",
    "        self.episode_steps = np.zeros(n)\n",
    "        self.n = n\n",
    "\n",
    "    def reset(self):\n",
    "        self.episode_reward = np.zeros(self.n)\n",
    "        self.episode_steps = np.zeros(self.n)\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, actions):\n",
    "        states, rewards, dones, infos = self.env.step(actions)\n",
    "        \n",
    "        mean_exploration_reward = np.mean(infos[0][\"exploration_rewards\"])\n",
    "        infos[0][\"mean_exploration_reward\"] = mean_exploration_reward\n",
    "        for i, done in enumerate(dones):\n",
    "            self.episode_steps[i] += 1\n",
    "            if done:\n",
    "                self.episode_reward[i] = rewards[i]\n",
    "                infos[i]['episode'] = {'r': self.episode_reward[i], 'steps': self.episode_steps[i]}\n",
    "                self.episode_steps[i] = 0\n",
    "                \n",
    "        return states, rewards, dones, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Linear\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        bert_model = 'bert-base-cased'\n",
    "        \n",
    "        self.bert_encoder = BertModel.from_pretrained(bert_model)\n",
    "        self.embed_dim = 768\n",
    "        self.output_size = 64\n",
    "        #self.out_layer = nn.Linear(self.embed_dim, self.output_size)\n",
    "        self.out_layer = nn.AdaptiveAvgPool1d(output_size=self.output_size)\n",
    "        tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "        self.pad_token = tokenizer.pad_token_id\n",
    "   \n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Encode batch of tokens\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_tokens_embed = self.bert_encoder(input_ids.long())[0]\n",
    "            out = self.out_layer(text_tokens_embed)\n",
    "            #out = relu(self.out_layer(text_tokens_embed))\n",
    "        \n",
    "        pad_mask = torch.where(input_ids.long() == self.pad_token, True, False)\n",
    "        return out, pad_mask\n",
    "\n",
    "\n",
    "class DOMBert(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        bert_model = 'bert-base-cased'\n",
    "        \n",
    "        self.bert_encoder = BertModel.from_pretrained(bert_model)\n",
    "        self.embed_dim = 768\n",
    "        self.output_size = 64 - 7\n",
    "        #self.out_layer = nn.Linear(self.embed_dim, self.output_size)\n",
    "        self.out_layer = nn.AdaptiveAvgPool1d(output_size=self.output_size)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Encode batch of tokens\n",
    "        \n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            text_tokens_embed = self.bert_encoder(input_ids.long())[1]\n",
    "            out = self.out_layer(text_tokens_embed)\n",
    "        #out = relu(self.out_layer(text_tokens_embed))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import ModuleList, Parameter, TransformerEncoderLayer, TransformerEncoder\n",
    "from torch.nn.functional import relu, softmax\n",
    "\n",
    "\n",
    "UNDIRECTED = True\n",
    "SELF_LOOP = True\n",
    "bert_model = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "class DOMEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DOMEncoder, self).__init__()\n",
    "        self.inp_dim = 64\n",
    "        self.output_size = 64\n",
    "        self.h_dim = 64\n",
    "        self.n_h_layers = 1\n",
    "        \n",
    "        self.layers = ModuleList([nn.Linear(self.inp_dim, self.h_dim)] + \\\n",
    "                                 [nn.Linear(self.h_dim, self.h_dim) for _ in range(self.n_h_layers)] + \\\n",
    "                                 [nn.Linear(self.h_dim, self.output_size)])\n",
    "        \n",
    "        self.bn = nn.ModuleList([nn.BatchNorm1d(self.inp_dim)] + \\\n",
    "                                [nn.BatchNorm1d(self.h_dim) for _ in range(self.n_h_layers + 1)])\n",
    "        \n",
    "        self.text_encoder = DOMBert()\n",
    "        self.tag_vectorizer = nn.Embedding(len(tags), 4)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        mask = input[\"dom_simple_feats\"][:, :, 0] # B x MAX_VERTS\n",
    "        mask = torch.where(mask==-1, True, False)\n",
    "        \n",
    "        tokens = input[\"dom_text_tokens\"]\n",
    "        bs = tokens.shape[0]\n",
    "        tokens = torch.flatten(tokens, end_dim=-2) # B*MAX_VERTS x MAX_TOKENS\n",
    "        text_feats = self.text_encoder(tokens)\n",
    "        \n",
    "        tag_feats = self.tag_vectorizer(input[\"dom_tags\"].long())\n",
    "        tag_feats = torch.flatten(tag_feats, end_dim=-2)   # B*MAX_VERTS x DIM_t\n",
    "        \n",
    "        feats = torch.flatten(input[\"dom_simple_feats\"], end_dim=-2) # B*MAX_VERTS x DIM_f\n",
    "        feats = torch.cat([text_feats, tag_feats, feats], dim=-1)\n",
    "        \n",
    "        out = feats\n",
    "        prev = feats\n",
    "        for l, b in zip(self.layers, self.bn):\n",
    "            out = b(out)\n",
    "            out = l(out)\n",
    "            out = F.relu(out)\n",
    "            out = out + prev\n",
    "            prev = out\n",
    "        out = out.view(bs, DOM_MAX_VERTICES, -1).permute([1, 0, 2]) # MAX_VERTS x B x DIM\n",
    "        return out, mask\n",
    "\n",
    "\n",
    "class DOMGoalTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DOMGoalTransformer, self).__init__()\n",
    "        self.inp_dim = 64\n",
    "        self.output_size = self.inp_dim\n",
    "        \n",
    "        self.embed_token = nn.Parameter(torch.empty(1, 1, self.inp_dim))\n",
    "        nn.init.uniform_(self.embed_token, -0.2, 0.2)\n",
    "        \n",
    "        encoder_layer = TransformerEncoderLayer(64, 4, dim_feedforward=64, norm_first=True)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, 4)\n",
    "    \n",
    "    def forward(self, inp, mask):\n",
    "        mask = torch.cat([torch.zeros_like(mask[:, :1], dtype=torch.bool), mask], dim=1)\n",
    "        inp = torch.cat([self.embed_token.repeat_interleave(inp.shape[1], dim=1), inp], dim=0)\n",
    "        out = self.encoder(inp, src_key_padding_mask=mask)\n",
    "        return out[0]\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dom = DOMEncoder()\n",
    "        self.goal = BertEncoder()\n",
    "        self.tf = DOMGoalTransformer()\n",
    "        self.output_size = 64\n",
    "    \n",
    "    def forward(self, input):\n",
    "        dom_embeds, dom_mask = self.dom(input)\n",
    "        goal_embeds, goal_mask = self.goal(input[\"goal_state\"])\n",
    "        dom_goal_embeds = torch.cat([dom_embeds, goal_embeds.permute([1, 0, 2])], dim=0)\n",
    "        dom_goal_mask = torch.cat([dom_mask, goal_mask], dim=1)\n",
    "        out = self.tf(dom_goal_embeds, dom_goal_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from rllr.models.ppo import FixedCategorical, ActorCriticNetwork, CriticNetwork\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class DiscreteActorNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor is a policy network. Given state it evaluates\n",
    "    probability of action given state or sample an action\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action_size, state_encoder, hidden_size):\n",
    "        super().__init__()\n",
    "        self.state_encoder = state_encoder\n",
    "        input_size = state_encoder.output_size\n",
    "        self.h_dim = hidden_size\n",
    "        self.inp_dim = input_size\n",
    "        self.output_size = action_size\n",
    "        self.n_h_layers = 1\n",
    "        \n",
    "        ####\n",
    "        dom_mask = torch.zeros((1, DOM_MAX_VERTICES), dtype=torch.bool)\n",
    "        words_mask = torch.ones((1, GOAL_MAX_WORDS), dtype=torch.bool)\n",
    "        words_mask[0, username_idx] = False; words_mask[0, passwd_idx] = False\n",
    "        words_mask = torch.cat([dom_mask, words_mask], dim=1)\n",
    "        self.register_buffer(\"words_mask\", words_mask)\n",
    "        ####\n",
    "        \n",
    "        self.layers = ModuleList([nn.Linear(self.inp_dim, self.h_dim)] + \\\n",
    "                                 [nn.Linear(self.h_dim, self.h_dim) for _ in range(self.n_h_layers)] +                                  [nn.Linear(self.h_dim, self.output_size)])\n",
    "        \n",
    "        self.bn = nn.ModuleList([nn.BatchNorm1d(self.inp_dim)] + \\\n",
    "                                [nn.BatchNorm1d(self.h_dim) for _ in range(self.n_h_layers+1)])\n",
    "\n",
    "    def forward(self, states):\n",
    "        states_encoding = self.state_encoder(states)\n",
    "        action_mask = states[\"action_mask\"]  # BxA\n",
    "        \n",
    "        ####\n",
    "        #bs = action_mask.shape[0]\n",
    "        #words_mask = torch.tile(self.words_mask, [bs, 1])\n",
    "        #action_mask = torch.logical_or(action_mask, words_mask)\n",
    "        ####\n",
    "        \n",
    "        for i, m in enumerate(action_mask):\n",
    "            if m.all():\n",
    "                action_mask[i] = torch.zeros_like(m, dtype=torch.bool)\n",
    "        \n",
    "        #logits = self.logits(states_encoding)\n",
    "        out = states_encoding\n",
    "        prev = states_encoding\n",
    "        for l, b in zip(self.layers[:-1], self.bn[:-1]):\n",
    "            out = b(out)\n",
    "            out = l(out)\n",
    "            out = F.relu(out)\n",
    "            out = out + prev\n",
    "            prev = out\n",
    "        out = self.bn[-1](out)\n",
    "        logits = self.layers[-1](out)\n",
    "        \n",
    "        logits = torch.where(action_mask==1, -float(\"inf\"), logits.double())\n",
    "        return FixedCategorical(logits=F.log_softmax(logits, dim=1))\n",
    "\n",
    "\n",
    "class ACN(ActorCriticNetwork):\n",
    "    def __init__(self, action_space, actor_state_encoder, critic_state_encoder, actor_hidden_size, critic_hidden_size):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        if type(action_space) == gym.spaces.Discrete:\n",
    "            self.actor = DiscreteActorNetwork(action_space.n, actor_state_encoder, actor_hidden_size)\n",
    "        else:\n",
    "            raise f'{action_space} not supported'\n",
    "        self.critic = CriticNetwork(critic_state_encoder, critic_hidden_size)\n",
    "\n",
    "        def init_params(m):\n",
    "            classname = m.__class__.__name__\n",
    "            if classname.find(\"Linear\") != -1:\n",
    "                m.weight.data.normal_(0, 1)\n",
    "                m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0)\n",
    "\n",
    "        self.apply(init_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rllr.env.vec_wrappers import make_vec_envs\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "\n",
    "def get_envs(n, **kwargs):\n",
    "    env = MiniWoBEnvironment(task_name, seeds=range(n), num_instances=n, base_url=base_url, **kwargs)\n",
    "    env = MiniWobClickElementWrapper(env)\n",
    "    return EpisodeInfoWrapper(env, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_instances = 4\n",
    "envs = get_envs(n_instances, headless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from rllr.models.encoders import GoalStateEncoder, SimpleCNN\n",
    "\n",
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rllr.algo.ppo.PPO at 0x7f6868472ca0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rllr.algo import PPO\n",
    "from rllr.models.ppo import ActorCriticNetwork\n",
    "\n",
    "hidden_size = 64\n",
    "policy = ACN(envs.action_space, encoder, encoder, hidden_size, hidden_size)\n",
    "\n",
    "agent_conf = {\n",
    "        \"clip_param\": 0.2,\n",
    "        \"ppo_epoch\": 4,\n",
    "        \"num_mini_batch\": 4,\n",
    "        \"value_loss_coef\": 0.5,\n",
    "        \"entropy_coef\": 0.01,\n",
    "        \"lr\": 0.001,\n",
    "        \"eps\": 1e-5,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "}\n",
    "\n",
    "\n",
    "agent = PPO(policy, **agent_conf)\n",
    "agent.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "import time\n",
    "from collections import deque\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from rllr.buffer.rollout import RolloutStorage\n",
    "\n",
    "\n",
    "def update_linear_schedule(optimizer, epoch, total_num_epochs, initial_lr):\n",
    "    \"\"\"Decreases the learning rate linearly\"\"\"\n",
    "    lr = initial_lr - (initial_lr * (epoch / float(total_num_epochs)))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def train_ppo(env, agent, conf):\n",
    "    \"\"\"\n",
    "    Runs a series of episode and collect statistics\n",
    "    \"\"\"\n",
    "    rollouts = RolloutStorage(\n",
    "        conf['training.n_steps'], conf['training.n_processes'], env.observation_space, env.action_space\n",
    "    )\n",
    "    obs = env.reset()\n",
    "    rollouts.set_first_obs(obs)\n",
    "    rollouts.to(conf['agent.device'])\n",
    "\n",
    "    start = time.time()\n",
    "    num_updates = int(conf['training.n_env_steps'] // conf['training.n_steps'] // conf['training.n_processes'])\n",
    "\n",
    "    episode_rewards = deque(maxlen=20)\n",
    "    exploration_rewards = deque(maxlen=100)\n",
    "    episode_steps = deque(maxlen=20)\n",
    "\n",
    "    for j in trange(num_updates):\n",
    "        update_linear_schedule(agent.optimizer, j, num_updates, conf['agent.lr'])\n",
    "\n",
    "        for step in range(conf['training.n_steps']):\n",
    "            # Sample actions\n",
    "            obs = {k: v.to(conf['agent.device']) for k, v in obs.items()}\n",
    "            value, action, action_log_prob = agent.act(obs)\n",
    "            obs, reward, done, infos = env.step(action)\n",
    "            \n",
    "            exploration_rewards.append(infos[0][\"mean_exploration_reward\"])\n",
    "            for info in infos:\n",
    "                if 'episode' in info.keys():\n",
    "                    episode_rewards.append(info['episode']['r'])\n",
    "                    episode_steps.append(info['episode']['steps'])\n",
    "\n",
    "            # If done then clean the history of observations.\n",
    "            masks = torch.FloatTensor([[0.0] if done_ else [1.0] for done_ in done])\n",
    "            rollouts.insert(obs, action, action_log_prob, value, reward, masks)\n",
    "\n",
    "        next_value = agent.get_value(rollouts.get_last_obs())\n",
    "        rollouts.compute_returns(next_value, conf['agent.gamma'], conf['agent.gae_lambda'])\n",
    "\n",
    "        value_loss, action_loss, dist_entropy = agent.update(rollouts)\n",
    "\n",
    "        if j % conf['training.verbose'] == 0 and len(episode_rewards) > 1:\n",
    "            total_num_steps = (j + 1) * conf['training.n_processes'] * conf['training.n_steps']\n",
    "            end = time.time()\n",
    "            print(f'Updates {j}, '\n",
    "                  f'num timesteps {total_num_steps}, '\n",
    "                  f'FPS {int(total_num_steps / (end - start))} \\n'\n",
    "                  f'Last {len(episode_rewards)} training episodes: '\n",
    "                  f'mean/median reward {np.mean(episode_rewards):.2f}/{np.median(episode_rewards):.2f}, '\n",
    "                  f'min/max reward {np.min(episode_rewards):.2f}/{np.max(episode_rewards):.2f}\\n'\n",
    "                  f'dist_entropy {dist_entropy:.2f}, '\n",
    "                  f'value_loss {value_loss:.2f}, '\n",
    "                  f'action_loss {action_loss:.2f}, '\n",
    "                  f'explor_rew {np.mean(exploration_rewards):.6f} '\n",
    "                  f'mean_episode_steps {np.mean(episode_steps):.2f}')\n",
    "            with open(conf['outputs.path']+\".txt\", 'a') as f:\n",
    "                f.write(f\"%d;%.4f;%.4f;%.6f;%.4f;%.4f\\n\" % \\\n",
    "                        (total_num_steps, np.mean(episode_rewards), \\\n",
    "                        np.median(episode_rewards), np.mean(exploration_rewards), \\\n",
    "                        dist_entropy, np.mean(episode_steps)))\n",
    "            \n",
    "        if j % 50 == 0:\n",
    "            torch.save(agent, conf['outputs.path']+\"_\"+str(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_conf = {\n",
    "    \"agent.lr\": 0.001,\n",
    "    \"agent.device\": device,\n",
    "    \"agent.gamma\": 0.99,\n",
    "    \"agent.gae_lambda\": 0.95,\n",
    "    \"training.n_env_steps\": 400000,\n",
    "    \"training.n_steps\": 100,\n",
    "    \"training.n_processes\": n_instances,\n",
    "    \"training.verbose\": 1,\n",
    "    \"outputs.path\": \"./miniwob_login\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_ppo(envs, agent, train_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f67cc51a610>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/d1cd98fa8509180c2a365dc0eb63ad96\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f68927b10d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/2fd89cd4d7ec16d8fe03efa88f3f7bb3\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f68910557f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/04fd8d941dc3c60187aed9e173e9d421\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f67cc51a5b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/e5d60024754f748ad15e8e0190bfb2ee\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f68910558b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/d1cd98fa8509180c2a365dc0eb63ad96\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f6868472370>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/2fd89cd4d7ec16d8fe03efa88f3f7bb3\n"
     ]
    }
   ],
   "source": [
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_instances = 4\n",
    "envs = get_envs(n_instances, headless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "import time\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def validate_ppo(env, conf):\n",
    "    agent = torch.load(conf['model_path'])\n",
    "    agent.to(conf['device'])\n",
    "    obs = env.reset()\n",
    "\n",
    "    start = time.time()\n",
    "    episode_rewards = deque(maxlen=conf['n_episodes'])\n",
    "    exploration_rewards = deque(maxlen=100)\n",
    "    episode_steps = deque(maxlen=conf['n_episodes'])\n",
    "\n",
    "    while len(episode_rewards) < conf['n_episodes']:\n",
    "        obs = {k: v.to(conf['device']) for k, v in obs.items()}\n",
    "        value, action, action_log_prob = agent.act(obs, deterministic=False)\n",
    "        obs, reward, done, infos = env.step(action)\n",
    "            \n",
    "        exploration_rewards.append(infos[0][\"mean_exploration_reward\"])\n",
    "        for info in infos:\n",
    "            if 'episode' in info.keys():\n",
    "                episode_rewards.append(info['episode']['r'])\n",
    "                episode_steps.append(info['episode']['steps'])\n",
    "        \n",
    "    print(f\"mean reward: %.2f\\tmedian reward: %.2f\\tmax reward: %.2f\\tmin reward: %.2f\\tmean steps per episode: %.2f\" % \\\n",
    "          (np.mean(episode_rewards), np.median(episode_rewards), np.max(episode_rewards), np.min(episode_rewards), np.mean(episode_steps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"model_path\": \"explore_solution_with_typeble_masking/miniwob_login_1600\",\n",
    "    \"n_episodes\": 100,\n",
    "    \"device\": device,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_ppo(envs, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f686960c580>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/b564a91f52da887ca9badd6f37f7d65b\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f68923bddc0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/424b4a8d414edf3c85d6cedb40388428\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f68923bd850>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/16ed54922e955db5a614df1993e83a1d\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f67cc649340>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/b564a91f52da887ca9badd6f37f7d65b\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f6869633bb0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/91fa18633076c6eefe399e68aae92fbc\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f686960c2e0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/424b4a8d414edf3c85d6cedb40388428\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f686960ce80>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/16ed54922e955db5a614df1993e83a1d\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f68dc7c92b0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/b564a91f52da887ca9badd6f37f7d65b\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f6869633850>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/91fa18633076c6eefe399e68aae92fbc\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f6869633a90>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/424b4a8d414edf3c85d6cedb40388428\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f6869633940>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/16ed54922e955db5a614df1993e83a1d\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f6869633190>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/91fa18633076c6eefe399e68aae92fbc\n"
     ]
    }
   ],
   "source": [
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
