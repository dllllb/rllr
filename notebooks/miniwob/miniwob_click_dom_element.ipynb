{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "device = \"cuda:1\"\n",
    "\n",
    "chrome_driver_path = \"/mnt/akostin/home/akostin/prjs/utils/chromedriver/\"\n",
    "app_path = \"/mnt/akostin/home/akostin/prjs/utils/chromedriver/\"\n",
    "os.environ[\"PATH\"] += os.pathsep + app_path\n",
    "os.environ[\"MINIWOB_BASE_URL\"] = \"file:///mnt/akostin/home/akostin/prjs/miniwob-plusplus/html/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE URL: file:///mnt/akostin/home/akostin/prjs/miniwob-plusplus/html/\n"
     ]
    }
   ],
   "source": [
    "from miniwob.environment import MiniWoBEnvironment\n",
    "from miniwob.screenshot import pil_to_numpy_array\n",
    "\n",
    "task_name = 'click-button'\n",
    "base_url = os.environ.get('MINIWOB_BASE_URL')\n",
    "print('BASE URL:', base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "X_MAX, Y_MAX = 160, 210\n",
    "DOM_TEXT_MAX_TOKENS = 8\n",
    "GOAL_MAX_TOKENS = 16\n",
    "DOM_MAX_EDGES = 20\n",
    "DOM_MAX_VERTICES = 20\n",
    "\n",
    "\n",
    "class DOMVertex:\n",
    "    def __init__(self, dom_element, i, text_tokenizer, tokenization_kwargs):\n",
    "        self.element = dom_element\n",
    "        self.id = i\n",
    "        self.tag = dom_element.tag\n",
    "        self.cx, self.cy, self.width, self.height = (dom_element.left + dom_element.width/2)/X_MAX, \\\n",
    "                                                    (dom_element.top + dom_element.height/2)/Y_MAX, \\\n",
    "                                                    dom_element.width/X_MAX, dom_element.height/Y_MAX\n",
    "        self.focused = dom_element.focused\n",
    "        self.tampered = dom_element.tampered\n",
    "        text = dom_element.text if dom_element.text else \"\"\n",
    "        self.text_tokens = text_tokenizer(text, **tokenization_kwargs)['input_ids']\n",
    "        self.isleaf = dom_element.is_leaf\n",
    "    \n",
    "    def get_feat_vector(self):\n",
    "        simple_feats = torch.tensor([self.cx, self.cy, self.width, self.height, self.focused, self.tampered],\\\n",
    "                                    dtype=torch.float32)\n",
    "        return simple_feats, self.text_tokens\n",
    "\n",
    "\n",
    "class DOMTree:\n",
    "    def __init__(self, text_tokenizer=None, tokenization_kwargs=dict()):\n",
    "        self.i=0\n",
    "        if text_tokenizer:\n",
    "            self.text_tokenizer = text_tokenizer\n",
    "            self.tokenization_kwargs = tokenization_kwargs\n",
    "        else:\n",
    "            self.text_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "            self.tokenization_kwargs ={\"padding\": 'max_length',\n",
    "                                       \"max_length\": DOM_TEXT_MAX_TOKENS,\n",
    "                                       \"truncation\": True,\n",
    "                                       \"return_tensors\": \"pt\",}\n",
    "        self.vertices = list()\n",
    "        self.fr0m = list()\n",
    "        self.to = list()\n",
    "        self.leafs = list()\n",
    "    \n",
    "    def build_tree(self, root_element):\n",
    "        candidates = [(root_element, -1)]\n",
    "        while len(candidates):\n",
    "            element, parent_id = candidates.pop()\n",
    "            self.vertices.append(DOMVertex(element, self.i, self.text_tokenizer, self.tokenization_kwargs))\n",
    "            if element.is_leaf:\n",
    "                self.leafs.append(self.vertices[-1])\n",
    "            if parent_id>=0:\n",
    "                self.fr0m.append(parent_id)\n",
    "                self.to.append(self.i)\n",
    "            candidates += [(child, self.i) for child in element.children]\n",
    "            self.i += 1\n",
    "    \n",
    "    def reset(self):\n",
    "        self.vertices = list()\n",
    "        self.fr0m = list()\n",
    "        self.to = list()\n",
    "        self.i = 0\n",
    "    \n",
    "    def get_data(self):\n",
    "        simple, text = list(), list()\n",
    "        for vertex in self.vertices:\n",
    "            s, t = vertex.get_feat_vector()\n",
    "            simple.append(s); text.append(t)\n",
    "        \n",
    "        delta_e = DOM_MAX_EDGES - len(self.fr0m)\n",
    "        self.fr0m += [-1 for i in range(delta_e)]\n",
    "        self.to += [-1 for i in range(delta_e)]\n",
    "        \n",
    "        delta_v = DOM_MAX_VERTICES - len(simple)\n",
    "        simple += [-torch.ones((6,), dtype=torch.float32) for i in range(delta_v)]\n",
    "        text += [torch.zeros((1, DOM_TEXT_MAX_TOKENS), dtype=torch.int64) for i in range(delta_v)]\n",
    "        \n",
    "        return {\n",
    "            \"dom_from\": torch.tensor(self.fr0m, dtype=torch.int64).unsqueeze(0),\n",
    "            \"dom_to\": torch.tensor(self.to, dtype=torch.int64).unsqueeze(0),\n",
    "            \"dom_simple_feats\": torch.stack(simple, dim=0).unsqueeze(0),\n",
    "            \"dom_text_tokens\": torch.cat(text, dim=0).unsqueeze(0),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Cannot call click([11] button @ (2, 158) text='No' classes=[NO_CLASS]) on instance 0, which is already done\n",
      "WARNING:root:Cannot call click([11] button @ (2, 158) text='No' classes=[NO_CLASS]) on instance 0, which is already done\n",
      "WARNING:root:Cannot call click([7] button @ (2, 105) text='yes' classes=[NO_CLASS]) on instance 0, which is already done\n",
      "WARNING:root:Cannot call click([6] input_text @ (2, 84) value= classes=[NO_CLASS]) on instance 0, which is already done\n",
      "WARNING:root:Cannot call click([10] input_text @ (2, 137) value= classes=[NO_CLASS]) on instance 0, which is already done\n",
      "WARNING:root:Cannot call click([7] button @ (2, 105) text='yes' classes=[NO_CLASS]) on instance 0, which is already done\n",
      "WARNING:root:Cannot call click([8] button @ (41, 105) text='submit' classes=[NO_CLASS]) on instance 0, which is already done\n",
      "WARNING:root:Cannot call click([6] input_text @ (2, 84) value= classes=[NO_CLASS]) on instance 0, which is already done\n",
      "WARNING:root:Cannot call click([4] button @ (2, 52) text='Cancel' classes=[NO_CLASS]) on instance 0, which is already done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True -1.0\n",
      "True -1.0\n",
      "True -1.0\n",
      "True -1.0\n",
      "True -1.0\n",
      "True -1.0\n",
      "True -1.0\n",
      "True -1.0\n",
      "True -1.0\n",
      "True -1.0\n"
     ]
    }
   ],
   "source": [
    "from miniwob.action import  MiniWoBElementClick\n",
    "import numpy as np\n",
    "\n",
    "tree = DOMTree()\n",
    "tree.reset()\n",
    "env = MiniWoBEnvironment(task_name, seeds=[1], num_instances=1, base_url=base_url, headless=True)\n",
    "state = env.reset(record_screenshots=True, seeds=[1123])[0]\n",
    "tree.build_tree(state.dom)\n",
    "done = False\n",
    "\n",
    "for _ in range(10):\n",
    "    random_leaf = np.random.choice(tree.leafs).element\n",
    "    action = [MiniWoBElementClick(random_leaf)]\n",
    "    obs, reward, done, infos = env.step(action)\n",
    "    print(done[0], reward[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe92d559070>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/46cc0202c65c2c81a8096137eae306ba\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe99ec716d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/46cc0202c65c2c81a8096137eae306ba\n"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym miniwob wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe8cacb9700>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/46cc0202c65c2c81a8096137eae306ba\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "\n",
    "from miniwob.action import MiniWoBElementClick\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MiniWobClickElementWrapper(gym.Wrapper):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        bert_model = 'bert-base-cased'\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "        self.tree = DOMTree()\n",
    "        \n",
    "        self.observation_space = gym.spaces.Dict({\"goal_state\": gym.spaces.Box(0, \n",
    "                                                                              self.tokenizer.vocab_size,\n",
    "                                                                               (GOAL_MAX_TOKENS, ),\n",
    "                                                                              dtype=np.int64),\n",
    "                                                  \"img_state\": gym.spaces.Box(0,\n",
    "                                                                        255,\n",
    "                                                                        (150, 150, 3),\n",
    "                                                                        dtype=np.uint8),\n",
    "                                                  \"dom_simple_feats\": gym.spaces.Box(-1,\n",
    "                                                                                     1,\n",
    "                                                                                     (DOM_MAX_VERTICES, 6),\n",
    "                                                                                     dtype=np.float32),\n",
    "                                                  \"dom_text_tokens\": gym.spaces.Box(0,\n",
    "                                                                                    self.tokenizer.vocab_size,\n",
    "                                                                                    (DOM_MAX_VERTICES, DOM_TEXT_MAX_TOKENS),\n",
    "                                                                                    dtype=np.int64),\n",
    "                                                  \"dom_from\": gym.spaces.Box(-1,\n",
    "                                                                             DOM_MAX_VERTICES,\n",
    "                                                                             (DOM_MAX_EDGES, ),\n",
    "                                                                             dtype=np.int64),\n",
    "                                                  \"dom_to\": gym.spaces.Box(-1,\n",
    "                                                                           DOM_MAX_VERTICES,\n",
    "                                                                           (DOM_MAX_EDGES, ),\n",
    "                                                                           dtype=np.int64)})\n",
    "        \n",
    "        self.inst_verts = [None for _ in range(self.num_instances)]\n",
    "        self.dom_keys = [key for key in self.observation_space if key.startswith(\"dom\")]\n",
    "        self.action_space = gym.spaces.Discrete(DOM_MAX_VERTICES)\n",
    "    \n",
    "    def _ob_to_dom(self, ob, i):\n",
    "        max_length = DOM_TEXT_MAX_TOKENS\n",
    "        if ob is None:\n",
    "            self.inst_verts[i] = None\n",
    "            return {\n",
    "                \"dom_simple_feats\": -torch.ones((1, DOM_MAX_VERTICES, 6), dtype=torch.float32),\n",
    "                \"dom_text_tokens\": torch.zeros((1, DOM_MAX_VERTICES, DOM_TEXT_MAX_TOKENS), dtype=torch.int64),\n",
    "                \"dom_from\": -torch.ones((1, DOM_MAX_EDGES), dtype=torch.int64),\n",
    "                \"dom_to\": -torch.ones((1, DOM_MAX_EDGES), dtype=torch.int64)\n",
    "            }\n",
    "        else:\n",
    "            self.tree.reset()\n",
    "            self.tree.build_tree(ob.dom)\n",
    "            self.inst_verts[i] = self.tree.vertices\n",
    "            return self.tree.get_data()\n",
    "    \n",
    "    def _ob_to_token(self, ob):\n",
    "        max_length = GOAL_MAX_TOKENS\n",
    "        if ob is None:\n",
    "            return torch.zeros(1, max_length)\n",
    "        else: \n",
    "            return self.tokenizer(ob.tokens,  \n",
    "                                  padding='max_length', \n",
    "                                  max_length = max_length, truncation=True,          \n",
    "                                  return_tensors=\"pt\",\n",
    "                                  is_split_into_words=True)['input_ids']\n",
    "            \n",
    "    def _ob_to_image(self, ob):\n",
    "        if ob is None:\n",
    "            return torch.zeros(1, 150, 150, 3)\n",
    "        else:\n",
    "            return torch.tensor(pil_to_numpy_array(ob.screenshot.resize([150, 150]))).unsqueeze(0)\n",
    "        \n",
    "    \n",
    "    def _to_miniwob_actions(self, actions):\n",
    "        \n",
    "        actions = actions.squeeze()\n",
    "        miniwob_actions = []\n",
    "        \n",
    "        for i in range(self.num_instances):\n",
    "            if self.instances[i].get_metadata()['done']:\n",
    "                miniwob_actions.append(None)\n",
    "            else:\n",
    "                if self.inst_verts[i] is None:\n",
    "                    miniwob_actions.append(None)\n",
    "                else:\n",
    "                    v_n = actions[i].item()\n",
    "                    element = self.inst_verts[i][v_n].element\n",
    "                    miniwob_actions.append(MiniWoBElementClick(element))\n",
    "        return miniwob_actions\n",
    "\n",
    "    def observation(self, obs):\n",
    "        goals = torch.cat([self._ob_to_token(ob) for ob in obs], dim=0)\n",
    "        imgs = torch.cat([self._ob_to_image(ob) for ob in obs], dim=0)\n",
    "        doms = [self._ob_to_dom(obs[i], i) for i in range(len(obs))]\n",
    "        doms_states = {key: torch.cat([dom[key] for dom in doms], dim=0) for key in self.dom_keys}\n",
    "        return {'img_state': imgs, 'goal_state': goals, **doms_states}\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.observation(self.env.reset(record_screenshots=True))\n",
    "    \n",
    "    def step(self, actions):\n",
    "        miniwob_actions = self._to_miniwob_actions(actions)\n",
    "        obs, rewards, dones, infos = self.env.step(miniwob_actions)        \n",
    "        obs = self.observation(obs)\n",
    "        \n",
    "        for instance in self.env.instances:\n",
    "            if instance.get_metadata()['done']:\n",
    "                instance.begin_task()\n",
    "        \n",
    "        return obs, torch.tensor([rewards]).T, torch.tensor([dones]).T, infos['n']\n",
    "\n",
    "\n",
    "class EpisodeInfoWrapper(gym.Wrapper):\n",
    "    \n",
    "    def __init__(self, env, n):\n",
    "        super(EpisodeInfoWrapper, self).__init__(env)\n",
    "        self.episode_reward = np.zeros(n)\n",
    "        self.episode_steps = np.zeros(n)\n",
    "        self.n = n\n",
    "\n",
    "    def reset(self):\n",
    "        self.episode_reward = np.zeros(self.n)\n",
    "        self.episode_steps = np.zeros(self.n)\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, actions):\n",
    "        states, rewards, dones, infos = self.env.step(actions)\n",
    "        \n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                self.episode_reward[i] = rewards[i]\n",
    "                self.episode_steps[i] += 1\n",
    "                infos[i]['episode'] = {'r': self.episode_reward[i], 'steps': self.episode_steps[i]}\n",
    "                \n",
    "        return states, rewards, dones, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from torch import nn\n",
    "from torch.nn import Linear\n",
    "from torch.nn.functional import relu\n",
    "import torch\n",
    "\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        bert_model = 'bert-base-cased'\n",
    "        \n",
    "        self.bert_encoder = BertModel.from_pretrained(bert_model)\n",
    "        self.embed_dim = 768\n",
    "        self.output_size = 64\n",
    "        self.out_layer = nn.Linear(self.embed_dim, self.output_size)\n",
    "        tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "        self.pad_token = tokenizer.pad_token_id\n",
    "   \n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Encode batch of tokens\n",
    "        \n",
    "        \"\"\"\n",
    "        text_tokens_embed = self.bert_encoder(input_ids.long())[0]\n",
    "        with torch.no_grad():\n",
    "            embeds = relu(self.out_layer(text_tokens_embed))\n",
    "        \n",
    "        pad_mask = torch.where(input_ids.long() == self.pad_token, True, False)\n",
    "        return embeds, pad_mask\n",
    "\n",
    "\n",
    "class DOMBert(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        bert_model = 'bert-base-cased'\n",
    "        \n",
    "        self.bert_encoder = BertModel.from_pretrained(bert_model)\n",
    "        self.embed_dim = 768\n",
    "        self.output_size = 64 - 6\n",
    "        self.out_layer = nn.Linear(self.embed_dim, self.output_size)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Encode batch of tokens\n",
    "        \n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            text_tokens_embed = self.bert_encoder(input_ids.long())[1]\n",
    "        out = relu(self.out_layer(text_tokens_embed))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "from dgl.nn import GraphConv\n",
    "import torch\n",
    "from torch.nn import ModuleList, Parameter, TransformerEncoderLayer, TransformerEncoder\n",
    "from torch.nn.functional import relu, softmax\n",
    "\n",
    "\n",
    "UNDIRECTED = True\n",
    "SELF_LOOP = True\n",
    "bert_model = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        in_feats, h_dim, n_layers = 64, 64, 2\n",
    "        in_layer = GraphConv(in_feats, h_dim)\n",
    "        self.layers = ModuleList([in_layer, *[GraphConv(h_dim, h_dim) for i in range(n_layers-1)]])\n",
    "    \n",
    "    def forward(self, graph, features):\n",
    "        out = features\n",
    "        skip = features\n",
    "        for layer in self.layers:\n",
    "            out = layer(graph, out)\n",
    "            out = relu(out)\n",
    "            out += skip\n",
    "            skip = out\n",
    "        return graph\n",
    "\n",
    "\n",
    "class DOMNET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DOMNET, self).__init__()\n",
    "        self.gcn = GCN()\n",
    "        self.bert = DOMBert()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        graphs = list()\n",
    "        for fr0m, to, simple_feats, text_tokens in zip(input[\"dom_from\"], input[\"dom_to\"],\\\n",
    "                                                       input[\"dom_simple_feats\"], input[\"dom_text_tokens\"]):\n",
    "            \n",
    "            edge_mask = fr0m!=-1\n",
    "            vertex_mask = torch.all(simple_feats!=-torch.ones_like(simple_feats[0]), dim=1)\n",
    "            fr0m = fr0m[edge_mask]\n",
    "            to = to[edge_mask]\n",
    "            simple_feats = simple_feats[vertex_mask]\n",
    "            text_tokens = text_tokens[vertex_mask]\n",
    "            \n",
    "            text_embed = self.bert(text_tokens)\n",
    "            vertices_embeds = torch.cat([text_embed, simple_feats], dim=-1)\n",
    "            \n",
    "            graph = dgl.graph((fr0m.long(), to.long()))\n",
    "            graph.ndata['vertex_embed'] = vertices_embeds\n",
    "            if UNDIRECTED:\n",
    "                graph = dgl.add_reverse_edges(graph)\n",
    "            if SELF_LOOP:\n",
    "                graph = dgl.add_self_loop(graph)\n",
    "            graphs.append(graph)\n",
    "        \n",
    "        batch_graph = dgl.batch(graphs)\n",
    "        batch_graph = self.gcn(batch_graph, batch_graph.ndata['vertex_embed'])\n",
    "        \n",
    "        graphs = [graph.ndata[\"vertex_embed\"] for graph in dgl.unbatch(batch_graph)]\n",
    "        embeds, mask = self.collate_fn(graphs)\n",
    "        return embeds, mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(tensor_list):\n",
    "        vec_dim = tensor_list[0].shape[1]\n",
    "        max_n_vec = max([t.shape[0] for t in tensor_list])\n",
    "        bs = len(tensor_list)\n",
    "\n",
    "        pad_mask = torch.zeros((bs, max_n_vec), dtype=torch.bool).to(device)\n",
    "        pad_vector = torch.zeros((1, vec_dim)).to(device)\n",
    "\n",
    "        for i in range(bs):\n",
    "            delta = max_n_vec - tensor_list[i].shape[0]\n",
    "            pad = torch.repeat_interleave(pad_vector, delta, dim=0)\n",
    "            tensor_list[i] = torch.cat([tensor_list[i], pad], dim=0)\n",
    "            pad_mask[i, -delta:] = True\n",
    "\n",
    "        return torch.stack(tensor_list, dim=1), pad_mask\n",
    "\n",
    "\n",
    "class DOMGoalTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DOMGoalTransformer, self).__init__()\n",
    "        self.inp_dim = 64\n",
    "        self.output_size = self.inp_dim\n",
    "        \n",
    "        self.embed_token = nn.Parameter(torch.empty(1, 1, self.inp_dim))\n",
    "        nn.init.uniform_(self.embed_token, -0.1, 0.1)\n",
    "        \n",
    "        encoder_layer = TransformerEncoderLayer(64, 4, dim_feedforward=64, norm_first=True)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, 2)\n",
    "    \n",
    "    def forward(self, inp, mask):\n",
    "        mask = torch.cat([torch.zeros_like(mask[:, :1], dtype=torch.bool), mask], dim=1)\n",
    "        inp = torch.cat([self.embed_token.repeat_interleave(inp.shape[1], dim=1), inp], dim=0)\n",
    "        out = self.encoder(inp, src_key_padding_mask=mask)\n",
    "        return out[0]\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dom=DOMNET(), goal=BertEncoder(), tf=DOMGoalTransformer()):\n",
    "        super(Encoder, self).__init__()\n",
    "        #self.img_encoder = img\n",
    "        self.dom_net = dom\n",
    "        self.goal_bert = goal\n",
    "        self.tf = tf\n",
    "        self.output_size = 64# + self.img_encoder.output_size\n",
    "    \n",
    "    def forward(self, input):\n",
    "        dom_embeds, dom_mask = self.dom_net(input)\n",
    "        goal_embeds, goal_mask = self.goal_bert(input[\"goal_state\"])\n",
    "        dom_goal_embeds = torch.cat([dom_embeds, goal_embeds.permute([1, 0, 2])], dim=0)\n",
    "        dom_goal_mask = torch.cat([dom_mask, goal_mask], dim=1)\n",
    "        tf_out = self.tf(dom_goal_embeds, dom_goal_mask)\n",
    "        \n",
    "        #img_out = self.img_encoder(input[\"img_state\"])\n",
    "        \n",
    "        #out = torch.cat([tf_out, img_out], dim=-1)\n",
    "        out = tf_out\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from rllr.models.ppo import FixedCategorical, ActorCriticNetwork, CriticNetwork\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class DiscreteActorNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor is a policy network. Given state it evaluates\n",
    "    probability of action given state or sample an action\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action_size, state_encoder, vertex_encoder, hidden_size):\n",
    "        super().__init__()\n",
    "        self.state_encoder = state_encoder\n",
    "        self.vertex_encoder = vertex_encoder\n",
    "        input_size = state_encoder.output_size\n",
    "        fc_layers = []\n",
    "        if type(hidden_size) == int:\n",
    "            fc_layers += [\n",
    "                nn.Linear(input_size, hidden_size),\n",
    "                nn.ReLU(inplace=False),\n",
    "                nn.Linear(hidden_size, input_size)\n",
    "            ]\n",
    "        elif type(hidden_size) == list:\n",
    "            for hs in hidden_size:\n",
    "                fc_layers.append(nn.Linear(input_size, hs))\n",
    "                fc_layers.append(nn.ReLU(inplace=False))\n",
    "                input_size = hs\n",
    "            fc_layers.append(nn.Linear(input_size, action_size))\n",
    "        else:\n",
    "            AttributeError(f\"unknown type of {hidden_size} parameter\")\n",
    "\n",
    "        self.logits = nn.Sequential(*fc_layers)\n",
    "        self.output_size = action_size\n",
    "\n",
    "    def forward(self, states):\n",
    "        states_encoding = self.state_encoder(states)\n",
    "        states_encoding = self.logits(states_encoding) # BxH\n",
    "        vertex_encoding, vertex_mask = self.vertex_encoder(states)  # VxBxH ; BxV\n",
    "        \n",
    "        if vertex_encoding.shape[0] == 0:\n",
    "            return FixedCategorical(logits=F.log_softmax(torch.zeros((vertex_encoding.shape[1], 2)), dim=1))\n",
    "        \n",
    "        logits = torch.matmul(vertex_encoding.permute([1,0,2]), states_encoding.unsqueeze(-1)).squeeze(-1) #BxV\n",
    "        \n",
    "        for i, m in enumerate(vertex_mask):\n",
    "            if m.all():\n",
    "                vertex_mask[i] = torch.zeros_like(m, dtype=torch.bool)\n",
    "        \n",
    "        logits = torch.where(vertex_mask, -float(\"inf\"), logits.double())\n",
    "        return FixedCategorical(logits=F.log_softmax(logits, dim=1))\n",
    "\n",
    "\n",
    "class ACN(ActorCriticNetwork):\n",
    "    def __init__(self, action_space, actor_state_encoder, actor_vertex_encoder, critic_state_encoder, actor_hidden_size, critic_hidden_size):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        if type(action_space) == gym.spaces.Discrete:\n",
    "            self.actor = DiscreteActorNetwork(action_space.n, actor_state_encoder, actor_vertex_encoder, actor_hidden_size)\n",
    "        else:\n",
    "            raise f'{action_space} not supported'\n",
    "        self.critic = CriticNetwork(critic_state_encoder, critic_hidden_size)\n",
    "\n",
    "        def init_params(m):\n",
    "            classname = m.__class__.__name__\n",
    "            if classname.find(\"Linear\") != -1:\n",
    "                m.weight.data.normal_(0, 1)\n",
    "                m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0)\n",
    "\n",
    "        self.apply(init_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rllr.env.vec_wrappers import make_vec_envs\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "\n",
    "def get_envs(n, **kwargs):\n",
    "    env = MiniWoBEnvironment(task_name, seeds=range(n), num_instances=n, base_url=base_url, **kwargs)\n",
    "    env = MiniWobClickElementWrapper(env)\n",
    "    return EpisodeInfoWrapper(env, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_instances = 8\n",
    "envs = get_envs(n_instances, headless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from rllr.models.encoders import GoalStateEncoder, SimpleCNN\n",
    "\n",
    "conf = {\n",
    "    \"n_channels\": [32, 64, 64],\n",
    "    \"kernel_sizes\": [4, 4, 3],\n",
    "    \"strides\": [4, 2, 1],\n",
    "    \"hidden_layers_sizes\": [64]\n",
    "}\n",
    "\n",
    "actor_vertex_encoder = DOMNET()\n",
    "#img = SimpleCNN(grid_size=150, conf=conf)\n",
    "#encoder = Encoder(img)\n",
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rllr.algo.ppo.PPO at 0x7fe89c0ec850>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rllr.algo import PPO\n",
    "from rllr.models.ppo import ActorCriticNetwork\n",
    "\n",
    "hidden_size = 32\n",
    "policy = ACN(envs.action_space, encoder, actor_vertex_encoder, encoder, hidden_size, hidden_size)\n",
    "\n",
    "agent_conf = {\n",
    "        \"clip_param\": 0.2,\n",
    "        \"ppo_epoch\": 4,\n",
    "        \"num_mini_batch\": 4,\n",
    "        \"value_loss_coef\": 0.5,\n",
    "        \"entropy_coef\": 0.01,\n",
    "        \"lr\": 0.001,\n",
    "        \"eps\": 1e-5,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "}\n",
    "\n",
    "\n",
    "agent = PPO(policy, **agent_conf)\n",
    "agent.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from rllr.utils.training import train_ppo\n",
    "\n",
    "train_conf = {\n",
    "    \"agent.lr\": 0.001,\n",
    "    \"agent.device\": device,\n",
    "    \"agent.gamma\": 0.99,\n",
    "    \"agent.gae_lambda\": 0.95,\n",
    "    \"training.n_env_steps\": 100000,\n",
    "    \"training.n_steps\": 100,\n",
    "    \"training.n_processes\": n_instances,\n",
    "    \"training.verbose\": 1,\n",
    "    \"outputs.path\": \"miniwob_element_click\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_ppo(envs, agent, train_conf)\n",
    "except:\n",
    "    train_ppo(envs, agent, train_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
