{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6efb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from rllr.env import minigrid_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "420a833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_conf = {\n",
    "    #\"env_task\": \"MiniGrid-LavaCrossingS9N3-v0\",\n",
    "    \"env_task\": \"MiniGrid-Empty-8x8-v0\",\n",
    "    \"grid_size\": 8,\n",
    "    \"action_size\": 3,\n",
    "    \"rgb_image\": True,\n",
    "    \"norm_type\": \"unit_norm\",\n",
    "    \"tile_size\": 8,\n",
    "    \"agent_rnn_layers\": 2,\n",
    "    \"agent_rhs_size\": 128,\n",
    "    \"minigrid_max_steps\": 64,\n",
    "    \"random_start_pos\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3880f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = minigrid_envs.gen_wrapped_env(env_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6d25f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c84b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86ad03a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc38ec3d2e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb92a274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eddd1bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(n=2000):\n",
    "    data = list()\n",
    "    env = minigrid_envs.gen_wrapped_env(env_conf)\n",
    "    for _ in range(n):\n",
    "        obs = env.reset()\n",
    "        data.append(torch.Tensor(obs))\n",
    "    return data\n",
    "\n",
    "dataset = gather_data()\n",
    "dataset = TensorDataset(torch.stack(dataset, dim=0))\n",
    "data = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fb27481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rllr.models.encoders import Permute\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        n_channels = [16, 16]\n",
    "        kernel_size = [4, 3]\n",
    "        strides = [4, 1]\n",
    "        paddings = [0, 1]\n",
    "        \n",
    "        in_channels = 3\n",
    "        conv_layers = [Permute(0, 3, 1, 2)]\n",
    "        \n",
    "        for out_cannels, k_size, stride, pad in zip(n_channels, kernel_size, strides, paddings):\n",
    "            layer = nn.Conv2d(in_channels, out_cannels, k_size, stride, pad)\n",
    "            conv_layers.append(layer)\n",
    "            conv_layers.append(nn.LeakyReLU())\n",
    "            in_channels = out_cannels\n",
    "    \n",
    "        self.net = nn.Sequential(*conv_layers)\n",
    "    \n",
    "    def get_out_shape(self, input_shape):\n",
    "        x = torch.zeros(1, *input_shape)\n",
    "        return self.net(x).shape[1:]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        channels = []\n",
    "        mu_layers = []\n",
    "        std_layers = []\n",
    "        \n",
    "        for ch in channels:\n",
    "            mu_layer = nn.Linear(in_channels, ch)\n",
    "            mu_layers.append(mu_layer)\n",
    "            mu_layers.append(nn.LeakyReLU())\n",
    "            \n",
    "            std_layer = nn.Linear(in_channels, ch)\n",
    "            std_layers.append(std_layer)\n",
    "            std_layers.append(nn.LeakyReLU())\n",
    "            \n",
    "            in_channels = ch\n",
    "            \n",
    "        self.mu_net = nn.Sequential(*(mu_layers[:-1]))\n",
    "        self.std_net = nn.Sequential(*(std_layers[:-1]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu = self.mu_net(x)\n",
    "        log_std = self.std_net(x)\n",
    "        \n",
    "        x = self.reparametrize(mu, log_std)\n",
    "        \n",
    "        kl_loss = self.kl_loss(mu, log_std)\n",
    "        return x, mu, log_std, kl_loss\n",
    "    \n",
    "    def reparametrize(self, mu, log_std):\n",
    "        std = torch.exp(log_std / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        x = mu + std * eps\n",
    "        return x\n",
    "    \n",
    "    def kl_loss(self, mu, log_std):\n",
    "        kl = -0.5 * torch.mean(1 +\n",
    "                               2 * log_std -\n",
    "                               mu.pow(2) -\n",
    "                               (2 * log_std).exp())\n",
    "        return kl\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        n_channels = [16, 3]\n",
    "        kernel_size = [3, 4]\n",
    "        strides = [1, 4]\n",
    "        paddings = [1, 0]\n",
    "        \n",
    "        in_channels = 16\n",
    "        conv_layers = list()\n",
    "        \n",
    "        for out_cannels, k_size, stride, pad in zip(n_channels, kernel_size, strides, paddings):\n",
    "            layer = nn.ConvTranspose2d(in_channels, out_cannels, k_size, stride, pad)\n",
    "            conv_layers.append(layer)\n",
    "            conv_layers.append(nn.LeakyReLU())\n",
    "            in_channels = out_cannels\n",
    "        \n",
    "        conv_layers.pop(-1)\n",
    "        conv_layers.append(nn.Sigmoid())\n",
    "    \n",
    "        self.net = nn.Sequential(*conv_layers)\n",
    "    \n",
    "    def get_out_shape(self, input_shape):\n",
    "        x = torch.zeros(1, *input_shape)\n",
    "        return self.net(x).shape[1:]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eb28431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nle_toolbox.zoo.vq import VQVAEEmbeddings, VQVAEIntegerCodes, VQEMAUpdater, VQLossHelper\n",
    "from nle_toolbox.zoo.vq import VectorQuantizedVAE as VQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4288a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = 32\n",
    "latent_dim = 256\n",
    "in_channels = 4096\n",
    "\n",
    "\n",
    "class VQMLP(nn.Module):\n",
    "    def __init__(self, in_channels, latent_dim, num_embeddings):\n",
    "        super(VQMLP, self).__init__()\n",
    "        \n",
    "        start_ch = in_channels\n",
    "        channels = [latent_dim]\n",
    "        \n",
    "        self.encode_net = nn.Linear(in_channels, latent_dim)\n",
    "        self.decode_net = nn.Linear(latent_dim, in_channels)\n",
    "        \n",
    "        vq_layer = VQ(num_embeddings, latent_dim)\n",
    "        self.vq = VQVAEEmbeddings(vq_layer)\n",
    "        self.inds_net = VQVAEIntegerCodes(vq_layer)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encode_net(x.reshape(x.shape[0], -1))\n",
    "        vq = self.vq(latent)\n",
    "        return self.decode_net(vq)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        with torch.no_grad():\n",
    "            latent = self.encode_net(x.reshape(x.shape[0], -1))\n",
    "            vq = self.vq(latent)\n",
    "        return vq\n",
    "    \n",
    "    def decode(self, latent):\n",
    "        with torch.no_grad():\n",
    "            vq = self.vq(latent)\n",
    "            x = self.decode_net(vq)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.mid = VQMLP(in_channels, latent_dim, num_embeddings)\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def forward(self, img):\n",
    "        enc_img = self.encoder(img.float() / 255.)\n",
    "        self.enc_img_shape = enc_img.shape\n",
    "        \n",
    "        latent_code = self.mid(enc_img)\n",
    "        latent_code = latent_code.view(*self.enc_img_shape)\n",
    "        \n",
    "        restored_imgs = self.decoder(latent_code)\n",
    "        return restored_imgs\n",
    "    \n",
    "    def encode(self, img):\n",
    "        with torch.no_grad():\n",
    "            latent = self.encoder(img.float() / 255.)\n",
    "            vq = self.mid.encode(latent)\n",
    "        return vq\n",
    "    \n",
    "    def decode(self, latent):\n",
    "        with torch.no_grad():\n",
    "            latent = self.mid.decode(latent)\n",
    "            restored_img = self.decoder(latent)\n",
    "        return restored_img\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3b7deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQVAE()\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters())\n",
    "hlp = VQLossHelper(model, reduction='mean')\n",
    "ema = VQEMAUpdater(model, alpha=0.1, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fb45f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import defaultdict\n",
    "writer = SummaryWriter('temp_experiments/test_vq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42abc5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|███████████████▎            | 27434/50000 [119:06:50<17:07:59,  2.73s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "\n",
    "for epoch in trange(50000):\n",
    "    batch_data = defaultdict(list)\n",
    "    for imgs, in data:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with hlp, ema:\n",
    "            restored_imgs = model(imgs)\n",
    "        restore_loss = criterion(restored_imgs, imgs.float().permute(0, 3, 1, 2) / 255.)\n",
    "        vq_ell = sum(hlp.finish().values())\n",
    "        loss = restore_loss + vq_ell\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ema.step()\n",
    "        \n",
    "        batch_data['restore_loss'].append(restore_loss.item())\n",
    "        batch_data['entropy'].append(list(ema.entropy.values())[0])\n",
    "\n",
    "    writer.add_scalar('entropy', np.mean(batch_data['entropy']), epoch)\n",
    "    writer.add_scalar('restore_loss', np.mean(batch_data['restore_loss']), epoch)\n",
    "    writer.add_image('initial_img', imgs[0].float() / 255., epoch, dataformats='HWC')\n",
    "    writer.add_image('restored_img', restored_imgs[0], epoch, dataformats='CHW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.mid.decode_net[0].wrapped.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd3d77b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
