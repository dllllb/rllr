{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from gym_minigrid_navigation.utils import show_video\n",
    "from navigation_policy import gen_env, get_agent, run_episode, run_episodes\n",
    "\n",
    "from rewards import get_reward_function\n",
    "from utils import init_logger, switch_reproducibility_on, display_stats\n",
    "\n",
    "from expected_steps import ExpectedStepsAmountLeaner\n",
    "from rewards import ExpectedStepsAmountReward\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "init_logger('__main__')\n",
    "\n",
    "init_logger('dqn')\n",
    "init_logger('expected_steps')\n",
    "init_logger('navigation_policy')\n",
    "init_logger('gym_minigrid_navigation.environments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhocon import ConfigFactory\n",
    "\n",
    "config = ConfigFactory.parse_file('../conf/minigrid_dqn_draft.hocon')\n",
    "config['env']['video_path'] = '../outputs/video/'\n",
    "\n",
    "switch_reproducibility_on(config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### agent and steps amount model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-25 16:44:58,534 INFO    dqn                    : Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "agent = get_agent(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_function = get_reward_function(config)\n",
    "env = gen_env(config['env'], reward_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-25 16:46:15,056 INFO    navigation_policy      : Episode: 100. scores: -10.10, steps: 109.94\n",
      "2021-02-25 16:46:53,955 INFO    navigation_policy      : Episode: 200. scores: -4.41, steps: 54.10\n",
      "2021-02-25 16:48:25,281 INFO    navigation_policy      : Episode: 300. scores: -11.95, steps: 125.46\n",
      "2021-02-25 16:49:22,770 INFO    navigation_policy      : Episode: 400. scores: -6.62, steps: 75.45\n",
      "2021-02-25 16:50:45,861 INFO    navigation_policy      : Episode: 500. scores: -9.77, steps: 106.81\n",
      "2021-02-25 16:51:10,297 INFO    navigation_policy      : Episode: 600. scores: -2.13, steps: 31.43\n",
      "2021-02-25 16:51:31,151 INFO    navigation_policy      : Episode: 700. scores: -1.62, steps: 26.84\n",
      "2021-02-25 16:52:14,635 INFO    navigation_policy      : Episode: 800. scores: -4.62, steps: 56.19\n",
      "2021-02-25 16:52:22,067 INFO    navigation_policy      : Episode: 900. scores: 0.15, steps: 9.49\n",
      "2021-02-25 16:52:34,835 INFO    navigation_policy      : Episode: 1000. scores: -0.53, steps: 16.05\n",
      "2021-02-25 16:52:44,816 INFO    navigation_policy      : Episode: 1100. scores: -0.17, steps: 12.69\n",
      "2021-02-25 16:53:05,493 INFO    navigation_policy      : Episode: 1200. scores: -1.56, steps: 26.45\n",
      "2021-02-25 16:53:19,021 INFO    navigation_policy      : Episode: 1300. scores: -0.61, steps: 17.12\n",
      "2021-02-25 16:53:27,998 INFO    navigation_policy      : Episode: 1400. scores: -0.03, steps: 11.29\n",
      "2021-02-25 16:53:30,913 INFO    navigation_policy      : Episode: 1500. scores: 0.77, steps: 3.33\n",
      "2021-02-25 16:53:41,242 INFO    navigation_policy      : Episode: 1600. scores: -0.21, steps: 13.00\n",
      "2021-02-25 16:53:48,398 INFO    navigation_policy      : Episode: 1700. scores: 0.19, steps: 8.99\n",
      "2021-02-25 16:53:51,988 INFO    navigation_policy      : Episode: 1800. scores: 0.66, steps: 4.36\n",
      "2021-02-25 16:53:57,132 INFO    navigation_policy      : Episode: 1900. scores: 0.47, steps: 6.33\n",
      "2021-02-25 16:54:04,013 INFO    navigation_policy      : Episode: 2000. scores: 0.33, steps: 7.71\n",
      "2021-02-25 16:54:12,976 INFO    navigation_policy      : Episode: 2100. scores: -0.01, steps: 11.14\n",
      "2021-02-25 16:54:20,204 INFO    navigation_policy      : Episode: 2200. scores: 0.22, steps: 8.79\n",
      "2021-02-25 16:54:27,213 INFO    navigation_policy      : Episode: 2300. scores: 0.24, steps: 8.59\n",
      "2021-02-25 16:54:36,799 INFO    navigation_policy      : Episode: 2400. scores: -0.07, steps: 11.68\n",
      "2021-02-25 16:54:50,983 INFO    navigation_policy      : Episode: 2500. scores: -0.67, steps: 17.57\n",
      "2021-02-25 16:54:58,579 INFO    navigation_policy      : Episode: 2600. scores: 0.17, steps: 9.28\n",
      "2021-02-25 16:55:03,603 INFO    navigation_policy      : Episode: 2700. scores: 0.49, steps: 6.13\n",
      "2021-02-25 16:55:07,912 INFO    navigation_policy      : Episode: 2800. scores: 0.59, steps: 5.07\n",
      "2021-02-25 16:55:13,249 INFO    navigation_policy      : Episode: 2900. scores: 0.44, steps: 6.61\n",
      "2021-02-25 16:55:19,312 INFO    navigation_policy      : Episode: 3000. scores: 0.38, steps: 7.19\n"
     ]
    }
   ],
   "source": [
    "config['env']['goal_type'] = 'random'\n",
    "\n",
    "scores, steps = run_episodes(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    n_episodes=config['training.n_episodes'],\n",
    "    verbose=config['training.verbose'],\n",
    "    max_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-26 00:01:27,693 INFO    navigation_policy      : Episode: 100. scores: -2.71, steps: 37.67\n"
     ]
    }
   ],
   "source": [
    "config['env']['goal_type'] = 'random'\n",
    "\n",
    "reward_function = get_reward_function(config)\n",
    "env = gen_env(config['env'], reward_function)\n",
    "\n",
    "scores, steps = run_episodes(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    n_episodes=100,\n",
    "    train_mode=True,\n",
    "    verbose=config['training.verbose'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-25 19:37:43,167 INFO    gym_minigrid_navigation.environments   : From [2 3] to [4 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-2.4000000000000017, 35)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" autoplay \n",
       "                loop controls style=\"height: 400px;\">\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gen_env(config['env'], reward_function, verbose=True)\n",
    "print(run_episode(env, agent, train_mode=True))\n",
    "\n",
    "show_video(config['env.video_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4f76a9dad686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### steps amount model trainings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_steps_learner = ExpectedStepsAmountLeaner(config['expected_steps_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['env']['goal_type'] = 'random'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gen_env(config['env'], reward_function=lambda *args: 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_steps_learner.config['update_step'] = 10_000_000\n",
    "\n",
    "scores, steps = run_episodes(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    steps_learner=expected_steps_learner,\n",
    "    n_episodes=config['expected_steps_params.warm_up'],\n",
    "    agent_train_mode=False,\n",
    "    verbose=config['training.verbose'],\n",
    "    max_steps=config['expected_steps_params.warm_up_max_steps']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### agent training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_function = ExpectedStepsAmountReward(expected_steps_learner.model)\n",
    "env = gen_env(config['env'], reward_function=reward_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_steps_learner.config['update_step'] = 10_000_000\n",
    "\n",
    "scores, steps = run_episodes(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    steps_learner=expected_steps_learner,\n",
    "    n_episodes=config['training.n_episodes'],\n",
    "    verbose=config['training.verbose']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_stats(scores, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gen_env(config['env'], reward_function, verbose=True)\n",
    "print(run_episode(env, agent, train_mode=True))\n",
    "\n",
    "show_video(config['env.video_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.nan / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "a += np.nan\n",
    "a == np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhocon import ConfigFactory\n",
    "\n",
    "config = ConfigFactory.parse_file('../conf/minigrid_dqn_navigation_draft.hocon')\n",
    "config['env']['video_path'] = '../outputs/video/'\n",
    "\n",
    "switch_reproducibility_on(config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trainings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward_function = get_reward_function(config)\n",
    "reward_function = get_reward_function({'training.reward': 'explicit_pos_reward'})\n",
    "\n",
    "agent = get_agent(config)\n",
    "env = gen_env(config['env'], reward_function)\n",
    "scores, steps = run_episodes(env, agent, n_episodes=config['training.n_episodes'], verbose=config['training.verbose'])\n",
    "\n",
    "display_stats(scores, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gen_env(config['env'], reward_function, verbose=True)\n",
    "# print(run_episode(env, agent, train_mode=False))\n",
    "\n",
    "# show_video(config['env.video_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from fast_tensor_data_loader import FastTensorDataLoader\n",
    "from expected_steps import ExpectedStepsAmountModel\n",
    "from rewards import ExpectedStepsAmount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = config['env.grid_size'] * config['env'].get('tile_size', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_steps_learner = ExpectedStepsAmountModel(grid_size, config['training.reward_params'])\n",
    "agent = get_agent(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gen_env(config['env'], reward_function=lambda *args: 0)\n",
    "env.env.env.max_steps = 40  # TOBD: config\n",
    "expected_steps_learner.model.fc[-1].bias.data.fill_(20.)\n",
    "\n",
    "expected_steps_learner.reset_buffer()\n",
    "expected_steps_learner.collect_episodes(env, agent)\n",
    "expected_steps_learner.learn(verbose=True)\n",
    "\n",
    "arr = np.array([x for _, _, x in expected_steps_learner.buffer])\n",
    "logger.info(f'{np.quantile(arr, np.arange(0, 1, 0.1))}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = get_agent(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_reward_function = ExpectedStepsAmount(expected_steps_learner.model)\n",
    "env = gen_env(config['env'], reward_function=expected_reward_function)\n",
    "# env.env.env.max_steps = 50  # TOBD: config\n",
    "\n",
    "# agent.reset_buffer()\n",
    "scores, steps = run_episodes(env, agent, n_episodes=3000, verbose=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gen_env(config['env'], expected_reward_function, verbose=True)\n",
    "print(run_episode(env, agent, train_mode=True))\n",
    "\n",
    "show_video(config['env.video_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gen_env(config['env'], reward_function=lambda *args: 0)\n",
    "env.env.env.max_steps = 40  # TOBD: config\n",
    "expected_steps_learner.model.fc[-1].bias.data.fill_(20.)\n",
    "\n",
    "expected_steps_learner.reset_buffer()\n",
    "expected_steps_learner.collect_episodes(env, agent)\n",
    "expected_steps_learner.learn(verbose=True)\n",
    "\n",
    "arr = np.array([x for _, _, x in expected_steps_learner.buffer])\n",
    "logger.info(f'{np.quantile(arr, np.arange(0, 1, 0.1))}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_reward_function = ExpectedStepsAmount(expected_steps_learner.model)\n",
    "env = gen_env(config['env'], reward_function=expected_reward_function)\n",
    "# env.env.env.max_steps = 50  # TOBD: config\n",
    "\n",
    "agent.reset_buffer()\n",
    "scores, steps = run_episodes(env, agent, n_episodes=3000, verbose=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigFactory.parse_file('../conf/minigrid_dqn_draft.hocon')\n",
    "config['env']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def metric(model, buffer, device=torch.device('cuda')):\n",
    "    def _vstack(arr):\n",
    "        arr = np.vstack([np.expand_dims(x, axis=0) for x in arr])\n",
    "        return torch.from_numpy(arr).float()\n",
    "\n",
    "    states, goal_states, y = map(_vstack, zip(*buffer))\n",
    "    y = y.squeeze()\n",
    "    y = torch.clamp(y, max=20)  # !!! hot fix\n",
    "    model.eval()\n",
    "    \n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "    train_loader = FastTensorDataLoader(states, goal_states, y, batch_size=1000)\n",
    "\n",
    "    loss_sum = 0\n",
    "    for batch_state, batch_goal_state, batch_y in train_loader:\n",
    "        with torch.no_grad():\n",
    "            output = model(batch_state.to(device), batch_goal_state.to(device))\n",
    "            loss = loss_fn(output, batch_y.to(device))\n",
    "        loss_sum += loss.cpu().numpy().item()\n",
    "    return loss_sum / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from navigation_policy import *\n",
    "from expected_steps import ExpectedStepsAmountLearner\n",
    "from rewards import ExpectedStepsAmount\n",
    "from fast_tensor_data_loader import FastTensorDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigFactory.parse_file('../conf/minigrid_dqn_navigation_draft.hocon')\n",
    "config['env']['video_path'] = '../outputs/video/'\n",
    "\n",
    "grid_size = config['env.grid_size'] * config['env'].get('tile_size', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_steps_learner = ExpectedStepsAmountLearner(grid_size, config['training.reward_params'])\n",
    "\n",
    "env = gen_env(config['env'], reward_function=lambda *args: 0)\n",
    "env.env.env.max_steps = 20  # TOBD: config\n",
    "# expected_steps_learner.model.fc[-1].bias.data.fill_(10.)\n",
    "    \n",
    "expected_steps_learner.collect_episodes(env, agent)\n",
    "valid_buffer = expected_steps_learner.buffer\n",
    "\n",
    "print(len(valid_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    expected_steps_learner.reset_buffer()\n",
    "    expected_steps_learner.collect_episodes(env, agent)\n",
    "\n",
    "    expected_steps_learner.learn()\n",
    "\n",
    "    metric_valid = metric(expected_steps_learner.model, valid_buffer)\n",
    "    metric_train = metric(expected_steps_learner.model, expected_steps_learner.buffer)\n",
    "    mean_steps = sum([x for _, _, x in expected_steps_learner.buffer]) / len(expected_steps_learner.buffer)\n",
    "    print(f\"epochs {i}: metric_train = {metric_train :.2f}, metric_valid = {metric_valid :.2f}, mean_steps: {mean_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['training']['n_episodes'] = 3000\n",
    "\n",
    "expected_reward_function = ExpectedStepsAmount(expected_steps_learner.model)\n",
    "env = gen_env(config['env'], reward_function=expected_reward_function)\n",
    "\n",
    "new_agent = get_agent(config)\n",
    "scores, steps = run_episodes(env, new_agent, n_episodes=config['training.n_episodes'], verbose=config['training.verbose'])\n",
    "\n",
    "display_stats(scores, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gen_env(config['env'], reward_function, verbose=True)\n",
    "print(run_episode(env, new_agent, train_mode=False))\n",
    "\n",
    "show_video(config['env.video_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(steps[-300:])\n",
    "logger.info(f'{np.quantile(arr, np.arange(0, 1, 0.1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['training.reward_params']['buffer_size'] = 5_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_steps_learner = ExpectedStepsAmountLearner(grid_size, config['training.reward_params'])\n",
    "agent = get_agent(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gen_env(config['env'], reward_function=lambda *args: 0, verbose=False)\n",
    "env.env.env.max_steps = 4  # TOBD: config\n",
    "expected_steps_learner.model.fc[-1].bias.data.fill_(2.)\n",
    "\n",
    "expected_steps_learner.reset_buffer()\n",
    "expected_steps_learner.collect_episodes(env, agent)\n",
    "expected_steps_learner.learn(verbose=True)\n",
    "\n",
    "arr = np.array([x for _, _, x in expected_steps_learner.buffer])\n",
    "logger.info(f'{np.quantile(arr, np.arange(0, 1, 0.1))}')\n",
    "\n",
    "expected_reward_function = ExpectedStepsAmount(expected_steps_learner.model, 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gen_env(config['env'], reward_function=lambda *args: 0, verbose=True)\n",
    "state = env.reset()\n",
    "goal_state = env.env.env.goal_state\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "start_pos = (5, 3)\n",
    "ens_pos = (2, 3)\n",
    "state_ = np.copy(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state = np.copy(state_)\n",
    "state = np.copy(state_)\n",
    "expected_reward_function(state, next_state, goal_state), to_coords(state), to_coords(next_state), to_coords(goal_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.copy(next_state)\n",
    "next_state[4, 2] = next_state[5, 2]\n",
    "next_state[5, 2] = [1, 0, 0]\n",
    "\n",
    "expected_reward_function(state, next_state, goal_state), to_coords(state), to_coords(next_state), to_coords(goal_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.copy(next_state)\n",
    "next_state[3, 2] = next_state[4, 2]\n",
    "next_state[4, 2] = [1, 0, 0]\n",
    "\n",
    "expected_reward_function(state, next_state, goal_state), to_coords(state), to_coords(next_state), to_coords(goal_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.copy(next_state)\n",
    "next_state[3, 3] = next_state[3, 2]\n",
    "next_state[3, 2] = [1, 0, 0]\n",
    "\n",
    "expected_reward_function(state, next_state, goal_state), to_coords(state), to_coords(next_state), to_coords(goal_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.copy(next_state)\n",
    "next_state[3, 4] = next_state[3, 3]\n",
    "next_state[3, 3] = [1, 0, 0]\n",
    "\n",
    "expected_reward_function(state, next_state, goal_state), to_coords(state), to_coords(next_state), to_coords(goal_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_reward_function = ExpectedStepsAmount(expected_steps_learner.model, 2.5)\n",
    "env = gen_env(config['env'], reward_function=expected_reward_function)\n",
    "# env.env.env.max_steps = 50  # TOBD: config\n",
    "\n",
    "agent = get_agent(config)\n",
    "agent.reset_buffer()\n",
    "\n",
    "for x in range(40):\n",
    "    scores, steps = run_episodes(env, agent, n_episodes=25, verbose=25)\n",
    "    arr = np.array(steps)\n",
    "    logger.info(f'{np.quantile(arr, np.arange(0, 1, 0.1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_coords(state):\n",
    "    obj_pos = (state == 10).nonzero()\n",
    "    return obj_pos[0].item(), obj_pos[1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = expected_reward_function.model\n",
    "dist = model(expected_reward_function._to_torch(state), expected_reward_function._to_torch(goal_state))\n",
    "\n",
    "to_coords(state), to_coords(goal_state), dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_reward_function = ExpectedStepsAmount(expected_steps_learner.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state[:,:,0], next_state[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state = np.copy(state)\n",
    "\n",
    "next_state[1, 3] = 0\n",
    "next_state[5, 3] = np.array([10, 0, 0])\n",
    "expected_reward_function(state, next_state, goal_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state = np.copy(state)\n",
    "expected_reward_function(state, next_state, goal_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_state[12:16, 12:16, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.qnetwork_target.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = agent.qnetwork_target.master\n",
    "model.output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(states).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(state - goal_state) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state / np.linalg.norm(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state / "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
